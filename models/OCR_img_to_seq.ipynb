{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKBNUyP24FuC"
      },
      "outputs": [],
      "source": [
        "# !pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-hx1JYf_Cai",
        "outputId": "6ed8c74b-2a53-40d3-8033-5cbf381c60fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.27.1 rapidfuzz-3.13.0\n"
          ]
        }
      ],
      "source": [
        "!pip install Levenshtein"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kl3eL1eJR_o"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSXS7zPJmFc"
      },
      "source": [
        "## Imports, Load Sampled MJSynth, Setup Drive and Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7lTE5LQ4HA0",
        "outputId": "777783c4-c363-47ca-b7ac-8c0fa29e2249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import Levenshtein\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "from itertools import groupby\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWj9v1Nt3ZJl"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMzNH9RJ3Cok"
      },
      "outputs": [],
      "source": [
        "num_images = 25000\n",
        "model_selected = 2  # 1: CNN + BiLSTMS\n",
        "                    # 2: CNN + Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo-T-zYx_Qr1"
      },
      "source": [
        "## Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvlGkRTn_TEj"
      },
      "outputs": [],
      "source": [
        "# Utility function: Greedy CTC decoder to convert model output logits to readable text sequences\n",
        "def ctc_decoder(logits, char_list):\n",
        "    \"\"\"\n",
        "    Decodes logits using greedy CTC decoding by collapsing repeated characters and removing blanks.\n",
        "    Args:\n",
        "        logits: Tensor of shape (T, B, C) — time, batch, classes\n",
        "        char_list: list of characters (vocab)\n",
        "    Returns:\n",
        "        List[str]: decoded strings for each sample in the batch\n",
        "    \"\"\"\n",
        "    if logits.ndim == 3 and logits.shape[0] != logits.size(1):\n",
        "        pass  # shape validation; no operation needed here\n",
        "\n",
        "    preds = torch.argmax(logits, dim=2)  # take most probable class at each timestep → [T, B]\n",
        "    preds = preds.permute(1, 0)  # transpose to [B, T] for easier iteration per sample\n",
        "\n",
        "    blank_idx = len(char_list)  # index used to represent the CTC blank token\n",
        "    results = []\n",
        "    for seq in preds:\n",
        "        prev = None\n",
        "        chars = []\n",
        "        for idx in seq.tolist():\n",
        "            # skip if character is blank or repeated\n",
        "            if idx != prev and idx != blank_idx:\n",
        "                chars.append(char_list[idx])\n",
        "            prev = idx\n",
        "        results.append(''.join(chars))\n",
        "    return results\n",
        "\n",
        "\n",
        "# Utility function: Decodes a single ground truth tensor label into string format for comparison\n",
        "def decode_label(tensor_label, char_list):\n",
        "    \"\"\"\n",
        "    Converts a padded label tensor to string, ignoring blank tokens.\n",
        "    \"\"\"\n",
        "    blank_idx = len(char_list)\n",
        "    return ''.join(char_list[i] for i in tensor_label.tolist() if i != blank_idx)\n",
        "\n",
        "\n",
        "# Utility function: Computes normalized edit distance between two strings as a similarity metric\n",
        "def compute_edit_distance(pred, gt):\n",
        "    return 1 - SequenceMatcher(None, pred, gt).ratio()\n",
        "\n",
        "\n",
        "# Utility function: Calculates character-level accuracy across all predictions\n",
        "def character_accuracy(preds, gts):\n",
        "    \"\"\"\n",
        "    Returns the proportion of correctly predicted characters over all ground truths.\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        m = min(len(p), len(g))\n",
        "        correct += sum(p[i] == g[i] for i in range(m))\n",
        "        total += len(g)\n",
        "    return correct / total if total else 0\n",
        "\n",
        "\n",
        "# Utility function: Calculates word-level accuracy (exact match) across all predictions\n",
        "def word_accuracy(preds, gts):\n",
        "    \"\"\"\n",
        "    Returns the proportion of completely correct word predictions.\n",
        "    \"\"\"\n",
        "    correct = sum(p == g for p, g in zip(preds, gts))\n",
        "    return correct / len(gts) if gts else 0\n",
        "\n",
        "\n",
        "# Utility function: Visualizes sample predictions vs ground truths with decoded text and accuracy metrics\n",
        "def visualize_predictions(model, dataset, char_list, num_samples=4):\n",
        "    \"\"\"\n",
        "    Displays a few random samples with their ground truth and predicted text for quick qualitative evaluation.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=num_samples, shuffle=True,\n",
        "        collate_fn=TextRecognitionDataset.custom_collate_fn\n",
        "    )\n",
        "    images, labels = next(iter(loader))\n",
        "    images = images.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(images)  # model forward pass, expected shape [T, B, C]\n",
        "        print(\"Raw logits sample slice:\", logits[:5, 0, :10])  # inspect first few timesteps and classes\n",
        "        preds = ctc_decoder(logits, char_list)\n",
        "\n",
        "    gts = [decode_label(lbl, char_list) for lbl in labels]\n",
        "\n",
        "    plt.figure(figsize=(15, 4))\n",
        "    for i in range(num_samples):\n",
        "        img = images[i].cpu().squeeze().numpy()\n",
        "        plt.subplot(1, num_samples, i+1)\n",
        "        plt.imshow(img, cmap='gray')\n",
        "        plt.title(f\"GT:{gts[i]}\\nPR:{preds[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nSample Metrics:\")\n",
        "    print(f\"Char Acc: {character_accuracy(preds, gts):.4f}\")\n",
        "    print(f\"Word Acc: {word_accuracy(preds, gts):.4f}\")\n",
        "\n",
        "\n",
        "# Utility function: Counts total number of trainable parameters in the model for model size evaluation\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Utility function: Computes Character Error Rate (CER) as total edit distance normalized by total GT characters\n",
        "def character_error_rate(preds, gts):\n",
        "    total_edits, total_chars = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        total_edits += levenshtein_distance(p, g)\n",
        "        total_chars += len(g)\n",
        "    return total_edits / total_chars if total_chars else 0\n",
        "\n",
        "\n",
        "# Utility function: Computes word-level accuracy allowing up to k character differences (edit distance tolerance)\n",
        "def word_accuracy_at_k(preds, gts, k):\n",
        "    correct = sum(levenshtein_distance(p, g) <= k for p, g in zip(preds, gts))\n",
        "    return correct / len(gts) if gts else 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mChlcnGJLeVd"
      },
      "source": [
        "## Load Dataset into Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAvTBTSVFuDR"
      },
      "outputs": [],
      "source": [
        "# # Load MJSynth dataset (non-streaming)\n",
        "# ds = load_dataset(\"priyank-m/MJSynth_text_recognition\", split=\"train\")\n",
        "\n",
        "# # Shuffle and select num_images samples\n",
        "# sampled = ds.shuffle(seed=42).select(range(num_images))\n",
        "# assert len(sampled) == num_images, f\"Sampled {len(sampled)} images, expected {num_images}\"\n",
        "\n",
        "# # Define output paths with versioning\n",
        "# base_dir = \"/content/drive/MyDrive/mjsynth_sampled\"\n",
        "# images_dir = os.path.join(base_dir, \"images\")\n",
        "# os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "# label_txt_path = os.path.join(base_dir, f\"label_{num_images}.txt\")\n",
        "\n",
        "# # Save sampled images and label file\n",
        "# with open(label_txt_path, \"w\") as f:\n",
        "#     for idx, item in enumerate(sampled):\n",
        "#         image: Image.Image = item['image']\n",
        "#         label = item['label']\n",
        "\n",
        "#         filename = f\"img_{idx:05d}.jpg\"\n",
        "#         full_img_path = os.path.join(images_dir, filename)\n",
        "#         relative_path = os.path.relpath(full_img_path, base_dir)\n",
        "\n",
        "#         if not os.path.exists(full_img_path):\n",
        "#             image.save(full_img_path)\n",
        "\n",
        "#         f.write(f\"{relative_path}\\t{label}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXy0v1BJZ7_"
      },
      "source": [
        "# Pre - Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2exHiJrkDrMG"
      },
      "outputs": [],
      "source": [
        "class TextRecognitionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset class for OCR text recognition.\n",
        "    Loads image paths and labels from a file, processes images, encodes labels into integers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, label_file, transform=None, char_list=None, max_label_len=None):\n",
        "        \"\"\"\n",
        "        Initializes the dataset by loading file paths and labels.\n",
        "\n",
        "        Args:\n",
        "            label_file: Path to label.txt containing image paths and labels separated by tab.\n",
        "            transform: torchvision transforms to apply to each image.\n",
        "            char_list: predefined character vocabulary. If None, built from dataset labels.\n",
        "            max_label_len: maximum label length for padding. If None, inferred from dataset.\n",
        "        \"\"\"\n",
        "        # Load image paths and labels from file into a DataFrame\n",
        "        self.samples = pd.read_csv(label_file, sep=\"\\t\", names=[\"img_path\", \"label\"])\n",
        "        self.samples.dropna(inplace=True)  # remove rows with missing labels\n",
        "\n",
        "        self.transform = transform\n",
        "        self.label_file = label_file\n",
        "\n",
        "        # Build vocabulary if not provided\n",
        "        if char_list is None:\n",
        "            all_text = \"\".join(self.samples[\"label\"])\n",
        "            self.char_list = sorted(set(all_text))  # sorted list of unique characters in dataset\n",
        "        else:\n",
        "            self.char_list = char_list\n",
        "\n",
        "        # Determine maximum label length (used for padding)\n",
        "        self.max_label_len = max_label_len or self.samples[\"label\"].str.len().max()\n",
        "\n",
        "        # Pre-encode all labels to integer sequences\n",
        "        self.encoded_labels = self.samples[\"label\"].apply(self.encode_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Loads and returns a single sample (image tensor and encoded label tensor).\n",
        "        \"\"\"\n",
        "        img_path = self.samples.iloc[idx][\"img_path\"]\n",
        "        label = self.encoded_labels.iloc[idx]\n",
        "\n",
        "        # Build full image path relative to label file directory\n",
        "        image_path = os.path.join(os.path.dirname(self.label_file), img_path)\n",
        "        image = Image.open(image_path).convert(\"L\")  # load as grayscale\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def encode_label(self, text):\n",
        "        \"\"\"\n",
        "        Encodes a string label into a list of integer indices based on the character vocabulary.\n",
        "        Pads the encoded label with blank indices to match max_label_len.\n",
        "        \"\"\"\n",
        "        label = [self.char_list.index(c) for c in text]\n",
        "        padded = label + [len(self.char_list)] * (self.max_label_len - len(label))\n",
        "        return padded\n",
        "\n",
        "    def get_vocab(self):\n",
        "        \"\"\"\n",
        "        Returns the character vocabulary used for encoding.\n",
        "        \"\"\"\n",
        "        return self.char_list\n",
        "\n",
        "    def get_max_label_len(self):\n",
        "        \"\"\"\n",
        "        Returns the maximum label length (after padding).\n",
        "        \"\"\"\n",
        "        return self.max_label_len\n",
        "\n",
        "    @staticmethod\n",
        "    def custom_collate_fn(batch):\n",
        "        \"\"\"\n",
        "        Custom collate function for DataLoader.\n",
        "        Stacks images into a batch tensor and converts list of encoded labels into tensor.\n",
        "        \"\"\"\n",
        "        images, labels = zip(*batch)\n",
        "        images = torch.stack(images, 0)\n",
        "        labels = torch.tensor(labels, dtype=torch.long)\n",
        "        return images, labels\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3kALQsWKJZs"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkzB1pD3K8vV",
        "outputId": "f1eebad4-52cf-4805-b06d-37cda9f8c634"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32, 128])\n",
            "[12, 24, 30, 23, 29, 14, 27, 29, 14, 23, 24, 27, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n"
          ]
        }
      ],
      "source": [
        "# Define Image Transformations to ensure same image size\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "label_file = f\"/content/drive/MyDrive/mjsynth_sampled/label_{num_images}.txt\"\n",
        "dataset = TextRecognitionDataset(label_file, transform=transform)\n",
        "\n",
        "vocab = dataset.get_vocab()\n",
        "vocab_size = len(vocab)\n",
        "blank_token = vocab_size  # assign blank token index\n",
        "\n",
        "# Example sample\n",
        "image, label = dataset[0]\n",
        "print(image.shape)\n",
        "print(label) # Padded encoded label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90mAqWerLABi"
      },
      "source": [
        "## Example image and its label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "fr-lIZ08LDY8",
        "outputId": "22838a59-8c06-4b90-96c0-a9204cb095b7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Get a sample\n",
        "image_tensor, encoded_label = dataset[3]  # You can change the index here (e.g., dataset[10])\n",
        "\n",
        "# Convert tensor to image (CHW to HWC)\n",
        "image_np = image_tensor.squeeze().numpy()  # Shape: [32, 128] for grayscale\n",
        "\n",
        "pad_token = vocab_size  # padding uses this value\n",
        "\n",
        "# Remove padding tokens and decode\n",
        "decoded_label = ''.join([vocab[i] for i in encoded_label if i != pad_token])\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_np, cmap='gray')\n",
        "plt.title(f\"Label: {decoded_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpRnBFFGLEQ8"
      },
      "source": [
        "## Train-Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSGfkOGnFlVe"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and validation sets (70% train, 30% val)\n",
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "# Define DataLoader for training set with shuffling for batching and generalisation\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=TextRecognitionDataset.custom_collate_fn\n",
        ")\n",
        "\n",
        "# Define DataLoader for validation set without shuffling\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=TextRecognitionDataset.custom_collate_fn\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzPKZnuHFqLp"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1uy0YuZzGi6"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4B1x_cTeyxCg"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic convolutional block with optional residual connection.\n",
        "    Consists of Conv2d -> BatchNorm -> ReLU layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, residual=False):\n",
        "        super().__init__()\n",
        "        self.residual = residual and in_channels == out_channels  # enable residual only if dimensions match\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv(x)\n",
        "        if self.residual:\n",
        "            return out + x  # add input for residual connection\n",
        "        return out\n",
        "\n",
        "\n",
        "def build_cnn(nc):\n",
        "    \"\"\"\n",
        "    Builds a feature extraction CNN to reduce input height to 1 for CRNN models.\n",
        "\n",
        "    Args:\n",
        "        nc: number of input channels (e.g. 1 for grayscale images)\n",
        "    Returns:\n",
        "        nn.Sequential containing convolutional feature extractor\n",
        "\n",
        "    Input shape: (B, 1, H, W)\n",
        "    Output shape: (B, 512, 1, W')\n",
        "    \"\"\"\n",
        "    layers = [\n",
        "        ConvBlock(nc, 64, 3, 1, 1),\n",
        "        nn.MaxPool2d(2, 2),  # halve height\n",
        "\n",
        "        ConvBlock(64, 128, 3, 1, 1),\n",
        "        nn.MaxPool2d(2, 2),  # halve height again\n",
        "\n",
        "        ConvBlock(128, 256, 3, 1, 1),\n",
        "        ConvBlock(256, 256, 3, 1, 1),  # second 256 block; residual disabled to reduce memory\n",
        "\n",
        "        nn.MaxPool2d((2, 1)),  # reduce height further, keep width\n",
        "\n",
        "        ConvBlock(256, 512, 3, 1, 1),\n",
        "        nn.BatchNorm2d(512),\n",
        "        nn.ReLU(inplace=True),\n",
        "\n",
        "        nn.MaxPool2d((2, 1)),  # final pooling along height\n",
        "\n",
        "        nn.AdaptiveAvgPool2d((1, None))  # ensure output height = 1, width preserved\n",
        "    ]\n",
        "\n",
        "    return nn.Sequential(*layers)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnFRQT6vQUSw"
      },
      "source": [
        "## CRNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlZ4ack_1Vpp"
      },
      "source": [
        "### Option 1: CNN + BiLSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vogKNVK-FsT1"
      },
      "outputs": [],
      "source": [
        "class CRNN_BiLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    CRNN model with CNN feature extractor and bidirectional LSTM for sequence modeling.\n",
        "    Suitable for OCR tasks with CTC decoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_h, nc, nclass, nh=256, num_rnn_layers=2):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_h: input image height (unused, kept for compatibility)\n",
        "            nc: number of input channels (1 for grayscale)\n",
        "            nclass: number of output classes (characters + blank)\n",
        "            nh: hidden size of LSTM\n",
        "            num_rnn_layers: number of stacked LSTM layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = build_cnn(nc)\n",
        "\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,       # CNN output channels\n",
        "            hidden_size=nh,\n",
        "            num_layers=num_rnn_layers,\n",
        "            bidirectional=True    # captures both left-to-right and right-to-left dependencies\n",
        "        )\n",
        "        self.embedding = nn.Linear(nh * 2, nclass)  # combines both directions to output class logits\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # shape: (B, 512, 1, W)\n",
        "        b, c, h, w = conv.size()\n",
        "        assert h == 1, f\"Expected height=1 after CNN, got {h}\"\n",
        "\n",
        "        conv = conv.squeeze(2).permute(2, 0, 1)  # reshape to (W, B, 512) for LSTM input\n",
        "\n",
        "        rnn_out, _ = self.rnn(conv)\n",
        "        out = self.embedding(rnn_out)  # shape: (W, B, nclass)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPAneJUNyyWT"
      },
      "source": [
        "### Option 2: CNN + Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Uj8Ys_zB5L"
      },
      "outputs": [],
      "source": [
        "class CRNN_Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    CRNN model with CNN feature extractor and multi-head self-attention for sequence modeling.\n",
        "    Uses causal mask to preserve autoregressive property required by CTC decoding.\n",
        "    \"\"\"\n",
        "    def __init__(self, img_h, nc, nclass, nh=256, num_heads=1, dropout=0.1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img_h: input image height (unused, kept for compatibility)\n",
        "            nc: number of input channels\n",
        "            nclass: number of output classes\n",
        "            nh: hidden size of attention\n",
        "            num_heads: number of attention heads\n",
        "            dropout: dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn = build_cnn(nc)\n",
        "        self.proj = nn.Linear(512, nh)  # project CNN output channels to attention hidden size\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(nh)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim=nh, num_heads=num_heads, dropout=dropout, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.embedding = nn.Linear(nh, nclass)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # (B, 512, 1, W)\n",
        "        b, c, h, w = conv.size()\n",
        "        assert h == 1, f\"Expected height=1 after CNN, got {h}\"\n",
        "\n",
        "        conv = conv.squeeze(2).permute(0, 2, 1)   # reshape to (B, W, 512)\n",
        "        conv_proj = self.proj(conv)               # project to (B, W, nh)\n",
        "\n",
        "        x_norm = self.norm1(conv_proj)            # layer normalization\n",
        "\n",
        "        seq_len = conv_proj.size(1)\n",
        "        # Create causal mask to prevent attention to future positions\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool().to(x.device)\n",
        "\n",
        "        attn_out, _ = self.attn(x_norm, x_norm, x_norm, attn_mask=mask)  # self-attention output (B, W, nh)\n",
        "        attn_out = self.dropout(attn_out)\n",
        "\n",
        "        x_res = conv_proj + attn_out             # add residual connection\n",
        "        out = self.embedding(x_res)              # final class logits (B, W, nclass)\n",
        "        return out.permute(1, 0, 2)              # reshape to (W, B, nclass) for CTC decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QDT1mQ7AFZV"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPe-Gtax_lel",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f9f0a9ec-41ac-442c-8a77-53cadbcef6e2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33madriabuil\u001b[0m (\u001b[33madriabuil-upc\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.21.0"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250710_145013-ua84zwtj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/adriabuil-upc/OCR/runs/ua84zwtj' target=\"_blank\">Image2Seq: CRNN-Attention-25000_imgs</a></strong> to <a href='https://wandb.ai/adriabuil-upc/OCR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/adriabuil-upc/OCR' target=\"_blank\">https://wandb.ai/adriabuil-upc/OCR</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/adriabuil-upc/OCR/runs/ua84zwtj' target=\"_blank\">https://wandb.ai/adriabuil-upc/OCR/runs/ua84zwtj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/adriabuil-upc/OCR/runs/ua84zwtj?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x79fa1e598a90>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Define model architecture name and WandB experiment name based on selected model\n",
        "if model_selected == 1:\n",
        "    architecture = \"CRNN (CNN + BiLSTM + CTC)\"\n",
        "    wandb_name = f\"Image2Seq: CRNN-BiLSTM-{num_images}_imgs\"\n",
        "    artifact_name = f\"best_model_CRNN_BiLSTM_{num_images}_imgs.pth\"\n",
        "else:\n",
        "    architecture = \"CRNN (CNN + Attention + CTC)\"\n",
        "    wandb_name = f\"Image2Seq: CRNN-Attention-{num_images}_imgs\"\n",
        "    artifact_name = f\"best_model_CRNN_Attention_{num_images}_imgs.pth\"\n",
        "\n",
        "# Configuration dictionary for WandB\n",
        "config = {\n",
        "    \"epochs\": 15,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"architecture\": architecture\n",
        "}\n",
        "\n",
        "# Initialize WandB logging\n",
        "wandb.init(project=\"OCR\", name=wandb_name, config=config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqAPdpOUFvgN"
      },
      "source": [
        "## Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-iS1OkuFx_8"
      },
      "outputs": [],
      "source": [
        "# Instantiate model based on selected architecture\n",
        "if model_selected == 1:\n",
        "    model = CRNN_BiLSTM(\n",
        "        img_h=32,\n",
        "        nc=1,  # number of input channels (1 for grayscale)\n",
        "        nclass=vocab_size + 1,  # +1 to include blank token\n",
        "        nh=256,\n",
        "        num_rnn_layers=2\n",
        "    ).to(device)\n",
        "else:\n",
        "    model = CRNN_Attention(\n",
        "        img_h=32,\n",
        "        nc=1,\n",
        "        nclass=vocab_size + 1,\n",
        "        nh=256,\n",
        "        num_heads=1\n",
        "    ).to(device)\n",
        "\n",
        "# Define CTC Loss function for sequence alignment without explicit segmentation\n",
        "criterion = nn.CTCLoss(blank=blank_token, zero_infinity=True)\n",
        "\n",
        "# Define optimizer for model training\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Define learning rate scheduler to reduce LR every 5 epochs by factor 0.5 for stable convergence\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77H1TEhiFzPQ"
      },
      "source": [
        "## Train one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWFN1ttSF1BW"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    \"\"\"\n",
        "    Trains the model for one epoch on the provided dataloader.\n",
        "\n",
        "    Args:\n",
        "        model: PyTorch model to train\n",
        "        dataloader: DataLoader providing (images, labels) batches\n",
        "        optimizer: optimizer used for weight updates\n",
        "        criterion: loss function (CTC Loss)\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: average training loss over the epoch\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # Wrap dataloader with tqdm to show progress bar during training\n",
        "    for imgs, labels in tqdm(dataloader, desc=\"Training\", leave=False):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        preds = model(imgs)  # output logits [T, B, C]\n",
        "        preds_log_softmax = preds.log_softmax(2)  # apply log-softmax over class dimension for CTC Loss\n",
        "\n",
        "        # Define input lengths (all equal to T since CNN output has fixed temporal length)\n",
        "        input_lengths = torch.full(\n",
        "            (preds.size(1),), preds.size(0), dtype=torch.long\n",
        "        ).to(device)\n",
        "\n",
        "        # Define target lengths by counting non-padding tokens in each label\n",
        "        target_lengths = torch.tensor(\n",
        "            [len(l[l != vocab_size]) for l in labels],\n",
        "            dtype=torch.long\n",
        "        ).to(device)\n",
        "\n",
        "        # Compute CTC loss between predicted sequence logits and ground truth labels\n",
        "        loss = criterion(preds_log_softmax, labels, input_lengths, target_lengths)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "\n",
        "    # Log average training loss to wandb for monitoring\n",
        "    wandb.log({\"Train loss\": avg_loss})\n",
        "\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-MUmhKkFr_1"
      },
      "source": [
        "## Sanity-check before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8we5tivEFDX_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "213a71f5-a361-48d3-f06b-6ac0729fe0fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Dataset Samples ==\n",
            "Sample 0: image shape=torch.Size([1, 32, 128]), label indices=[12, 24, 30, 23, 29, 14, 27, 29, 14, 23, 24, 27, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n",
            "Decoded label: COUNTERTENOR\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1: image shape=torch.Size([1, 32, 128]), label indices=[25, 53, 44, 48, 36, 38, 60, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62]\n",
            "Decoded label: Primacy\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "== Dataloader Batch Validation ==\n",
            "Labels max value in batch: 62\n",
            "Allowed max index (blank token): 62\n"
          ]
        }
      ],
      "source": [
        "def validate_dataset_and_loader(dataset, dataloader, vocab, num_samples):\n",
        "    \"\"\"\n",
        "    Performs two essential sanity checks before training:\n",
        "\n",
        "    1. Displays a few samples from the dataset with image shapes, encoded labels, and decoded text\n",
        "       to verify correct loading, encoding, and image preprocessing.\n",
        "    2. Validates a batch from the DataLoader to ensure that all label indices are within\n",
        "       the valid CTC range (including the blank token).\n",
        "\n",
        "    Args:\n",
        "        dataset: your dataset object supporting indexing\n",
        "        dataloader: your DataLoader object\n",
        "        vocab: list of characters representing your vocabulary (excluding blank token)\n",
        "        num_samples: number of samples to visualize from the dataset\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"== Dataset Samples ==\")\n",
        "    for i in range(num_samples):\n",
        "        img, encoded_label = dataset[i]\n",
        "        print(f\"Sample {i}: image shape={img.shape}, label indices={encoded_label}\")\n",
        "\n",
        "        # Decode label indices into string, skipping blank padding\n",
        "        decoded_label = ''.join([vocab[idx] for idx in encoded_label if idx != blank_token])\n",
        "        print(f\"Decoded label: {decoded_label}\")\n",
        "\n",
        "        plt.imshow(img.squeeze().numpy(), cmap='gray')\n",
        "        plt.title(f\"Label: {decoded_label}\")\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    print(\"\\n== Dataloader Batch Validation ==\")\n",
        "    for imgs, labels in dataloader:\n",
        "        max_label = labels.max().item()\n",
        "        print(f\"Labels max value in batch: {max_label}\")\n",
        "        print(f\"Allowed max index (blank token): {blank_token}\")\n",
        "\n",
        "        # Validate that no label index exceeds blank token index\n",
        "        assert max_label <= blank_token, \\\n",
        "            f\"Label index {max_label} out of range! Must be <= {blank_token} (vocab size: {vocab_size})\"\n",
        "        break  # only check first batch to save time\n",
        "\n",
        "# Example call before training to ensure data integrity\n",
        "validate_dataset_and_loader(dataset, train_loader, vocab, num_samples=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4kmC2CQwTD"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyOePsJMRNh3",
        "outputId": "34888335-477a-4bd2-8707-4d03d97dff05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:   2%|▏         | 9/547 [01:13<1:09:21,  7.73s/it]"
          ]
        }
      ],
      "source": [
        "best_loss = float('inf')  # track best validation loss for model checkpointing\n",
        "\n",
        "# Training loop over epochs\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "\n",
        "    # === Training ===\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # === Validation ===\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds, all_gts = [], []  # store all predictions and GTs for metric computation\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            preds = model(imgs)  # model forward pass\n",
        "            preds_log_softmax = preds.log_softmax(2)\n",
        "\n",
        "            # Define input lengths (all equal to T)\n",
        "            input_lengths = torch.full(\n",
        "                (preds.size(1),), preds.size(0), dtype=torch.long\n",
        "            ).to(device)\n",
        "\n",
        "            # Define target lengths (exclude padding tokens)\n",
        "            target_lengths = torch.tensor(\n",
        "                [len(l[l != blank_token]) for l in labels],\n",
        "                dtype=torch.long\n",
        "            ).to(device)\n",
        "\n",
        "            # Compute CTC loss\n",
        "            loss = criterion(preds_log_softmax, labels, input_lengths, target_lengths)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Decode predictions and ground truths for metrics\n",
        "            batch_preds = ctc_decoder(preds, vocab)\n",
        "            batch_gts = [decode_label(lbl, vocab) for lbl in labels]\n",
        "\n",
        "            all_preds += batch_preds\n",
        "            all_gts += batch_gts\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # === Character Error Rate (CER) ===\n",
        "    cer = character_error_rate(all_preds, all_gts)\n",
        "    print(f\"CER: {cer:.4f}\")\n",
        "\n",
        "    # === Word Accuracy@k ===\n",
        "    acc_k_dict = {}\n",
        "    for k in [0, 1, 2]:\n",
        "        acc_k = word_accuracy_at_k(all_preds, all_gts, k)\n",
        "        acc_k_dict[f\"Word Accuracy @{k}\"] = acc_k\n",
        "        print(f\"Word Accuracy @{k}: {acc_k:.4f}\")\n",
        "\n",
        "    # === Learning Rate Logging ===\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning Rate: {lr:.6f}\")\n",
        "\n",
        "    # === Log all metrics to WandB ===\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"Train Loss\": train_loss,\n",
        "        \"Val Loss\": val_loss,\n",
        "        \"CER\": cer,\n",
        "        **acc_k_dict,\n",
        "        \"LR\": lr,\n",
        "    })\n",
        "\n",
        "    # === Step the learning rate scheduler ===\n",
        "    scheduler.step()\n",
        "\n",
        "    # === Save best model checkpoint ===\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), artifact_name)\n",
        "\n",
        "        # Save best model as a wandb artifact for experiment tracking\n",
        "        artifact = wandb.Artifact(artifact_name, type='model')\n",
        "        artifact.add_file(artifact_name)\n",
        "        wandb.log_artifact(artifact)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJS8XMWeF4NO"
      },
      "source": [
        "# Inference & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-yLsXXt_82U"
      },
      "outputs": [],
      "source": [
        "visualize_predictions(model, dataset, dataset.get_vocab(), num_samples=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWzqRu27CMz"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FY665zHZABrB"
      },
      "outputs": [],
      "source": [
        "num_params = count_parameters(model)\n",
        "print(f\"Number of model parameters: {num_params:,}\")\n",
        "wandb.log({\"num_params\": num_params})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3skIYgaAMZe"
      },
      "source": [
        "## Final Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq2auCX27Pdr"
      },
      "outputs": [],
      "source": [
        "def final_precision_recall_auc(model, val_loader, char_list):\n",
        "    \"\"\"\n",
        "    Computes Precision-Recall curve and AUC, logs results to wandb,\n",
        "    and saves the PR plot as an image artifact.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            logits = model(images)\n",
        "            probs = torch.softmax(logits, dim=2)\n",
        "\n",
        "            preds = ctc_decoder(logits, char_list)\n",
        "            gts = [decode_label(lbl, char_list) for lbl in labels]\n",
        "\n",
        "            pred_ids = torch.argmax(logits, dim=2)\n",
        "            pred_probs = probs.gather(2, pred_ids.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "            pred_probs = pred_probs.permute(1,0)\n",
        "            mean_confidences = pred_probs.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            y_true.extend([int(p==g) for p,g in zip(preds,gts)])\n",
        "            y_scores.extend(mean_confidences)\n",
        "\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "    auc_score = auc(recall, precision)\n",
        "\n",
        "    # Plot and save the Precision-Recall curve\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(recall, precision, marker='.', label=f'AUC={auc_score:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save plot to temporary file and log as wandb artifact (image)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
        "        plt.savefig(tmpfile.name)\n",
        "        wandb.log({\"Precision-Recall Curve\": wandb.Image(tmpfile.name)})\n",
        "    plt.close()\n",
        "\n",
        "    # Log AUC value to wandb as a time-series metric for line chart display\n",
        "    wandb.log({\n",
        "        \"Precision-Recall AUC\": auc_score\n",
        "    })\n",
        "\n",
        "    print(f\"Final Precision-Recall AUC: {auc_score:.4f}\")\n",
        "    return auc_score\n",
        "\n",
        "\n",
        "\n",
        "def final_roc_auc(model, val_loader, char_list):\n",
        "    \"\"\"\n",
        "    Computes ROC curve and AUC, logs results to wandb,\n",
        "    and saves the ROC plot as an image artifact.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            logits = model(images)\n",
        "            probs = torch.softmax(logits, dim=2)\n",
        "\n",
        "            preds = ctc_decoder(logits, char_list)\n",
        "            gts = [decode_label(lbl, char_list) for lbl in labels]\n",
        "\n",
        "            pred_ids = torch.argmax(logits, dim=2)\n",
        "            pred_probs = probs.gather(2, pred_ids.unsqueeze(2)).squeeze(2)\n",
        "\n",
        "            pred_probs = pred_probs.permute(1,0)\n",
        "            mean_confidences = pred_probs.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            y_true.extend([int(p==g) for p,g in zip(preds,gts)])\n",
        "            y_scores.extend(mean_confidences)\n",
        "\n",
        "    # Compute ROC curve\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "    # Plot and save the ROC curve\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, marker='.', label=f'AUC={roc_auc:.2f}')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    # Save plot to temporary file and log as wandb artifact (image)\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
        "        plt.savefig(tmpfile.name)\n",
        "        wandb.log({\"ROC Curve\": wandb.Image(tmpfile.name)})\n",
        "    plt.close()\n",
        "\n",
        "    # Log AUC value to wandb\n",
        "    wandb.log({\n",
        "        \"ROC AUC\": roc_auc\n",
        "    })\n",
        "\n",
        "    print(f\"Final ROC AUC: {roc_auc:.4f}\")\n",
        "    return roc_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iagEGpIuARGV"
      },
      "outputs": [],
      "source": [
        "final_precision_recall_auc(model, val_loader, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwU9W2ESARVd"
      },
      "outputs": [],
      "source": [
        "final_roc_auc(model, val_loader, vocab)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}