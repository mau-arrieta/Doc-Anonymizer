{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKBNUyP24FuC"
      },
      "outputs": [],
      "source": [
        "# !pip install -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Levenshtein"
      ],
      "metadata": {
        "id": "rdfK-cuDAx01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kl3eL1eJR_o"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmSXS7zPJmFc"
      },
      "source": [
        "## Imports, Load Sampled MJSynth, Setup Drive and Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7lTE5LQ4HA0",
        "outputId": "58d129a9-76bf-4df3-e8a5-b7e48b275b54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tempfile\n",
        "import wandb\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "import Levenshtein\n",
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "from difflib import SequenceMatcher\n",
        "from itertools import groupby\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWj9v1Nt3ZJl"
      },
      "source": [
        "## Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMzNH9RJ3Cok"
      },
      "outputs": [],
      "source": [
        "num_images = 20000\n",
        "model_selected = 1  # 1: Encoder: CNN + Decoder: GRU -> Attention\n",
        "                    # 2: Encoder: CNN + Decoder: Attention -> GRU\n",
        "                    # 3: Encoder: CNN + Decoder: Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility Functions"
      ],
      "metadata": {
        "id": "77D0MxwEA7N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Transformer Masking ===\n",
        "def generate_square_subsequent_mask(sz, device=None):\n",
        "    \"\"\"\n",
        "    Creates an upper-triangular matrix filled with -inf (masked), used for autoregressive decoding.\n",
        "    Unmasked elements are 0.0.\n",
        "    Used to prevent the decoder from attending to future positions.\n",
        "    \"\"\"\n",
        "    return torch.triu(torch.full((sz, sz), float('-inf'), device=device), diagonal=1)\n",
        "\n",
        "\n",
        "# === Sequence Decoding Utilities ===\n",
        "def decode_sequence(sequence, inv_vocab, eos_token=\"<EOS>\", skip_tokens={\"<SOS>\", \"<PAD>\"}):\n",
        "    \"\"\"\n",
        "    Decodes a single token sequence to string, stopping at <EOS> and skipping control tokens.\n",
        "    \"\"\"\n",
        "    chars = []\n",
        "    for idx in sequence:\n",
        "        token = inv_vocab.get(idx.item() if isinstance(idx, torch.Tensor) else idx, \"\")\n",
        "        if token == eos_token:\n",
        "            break\n",
        "        if token not in skip_tokens:\n",
        "            chars.append(token)\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def decode_ground_truth(label_tensor, inv_vocab, sos_token=\"<SOS>\", eos_token=\"<EOS>\", pad_token=\"<PAD>\"):\n",
        "    \"\"\"\n",
        "    Decodes a ground truth label tensor into readable text.\n",
        "    Skips padding and special tokens.\n",
        "    \"\"\"\n",
        "    chars = []\n",
        "    for idx in label_tensor:\n",
        "        char = inv_vocab.get(idx.item(), \"\")\n",
        "        if char in [sos_token, pad_token]:\n",
        "            continue\n",
        "        if char == eos_token:\n",
        "            break\n",
        "        chars.append(char)\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def decode_label(tensor_label, char_list):\n",
        "    \"\"\"\n",
        "    Simple decode of a tensor label using index lookup.\n",
        "    Removes special tokens like <PAD>, <SOS>, <EOS>.\n",
        "    \"\"\"\n",
        "    special_tokens = {\"<PAD>\", \"<SOS>\", \"<EOS>\"}\n",
        "    return ''.join(\n",
        "        c for i in tensor_label.tolist()\n",
        "        if (c := char_list[i]) not in special_tokens\n",
        "    )\n",
        "\n",
        "# Utility function: Computes normalized edit distance between two strings as a similarity metric\n",
        "def compute_edit_distance(pred, gt):\n",
        "    return 1 - SequenceMatcher(None, pred, gt).ratio()\n",
        "\n",
        "\n",
        "# Utility function: Calculates character-level accuracy across all predictions\n",
        "def character_accuracy(preds, gts):\n",
        "    \"\"\"\n",
        "    Returns the proportion of correctly predicted characters over all ground truths.\n",
        "    \"\"\"\n",
        "    correct, total = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        m = min(len(p), len(g))\n",
        "        correct += sum(p[i] == g[i] for i in range(m))\n",
        "        total += len(g)\n",
        "    return correct / total if total else 0\n",
        "\n",
        "\n",
        "# Utility function: Calculates word-level accuracy (exact match) across all predictions\n",
        "def word_accuracy(preds, gts):\n",
        "    \"\"\"\n",
        "    Returns the proportion of completely correct word predictions.\n",
        "    \"\"\"\n",
        "    correct = sum(p == g for p, g in zip(preds, gts))\n",
        "    return correct / len(gts) if gts else 0\n",
        "\n",
        "\n",
        "def visualize_predictions(encoder, decoder, dataset, token2idx, num_samples):\n",
        "    \"\"\"\n",
        "    Visualizes model predictions for a few random samples from the dataset.\n",
        "    Displays GT vs Prediction for each image, plus accuracy metrics.\n",
        "    \"\"\"\n",
        "    if isinstance(token2idx, list):\n",
        "        token2idx = {token: idx for idx, token in enumerate(token2idx)}\n",
        "\n",
        "    inv_vocab = {idx: char for char, idx in token2idx.items()}\n",
        "    sos_token_idx = token2idx.get(\"<SOS>\", 0)\n",
        "    eos_token_idx = token2idx.get(\"<EOS>\", -1)\n",
        "\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "\n",
        "    dataloader = DataLoader(dataset, batch_size=num_samples, shuffle=True, collate_fn=collate_fn_wrapper(dataset))\n",
        "\n",
        "    images, labels = next(iter(dataloader))\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    batch_size = images.size(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        encoder_outputs = encoder(images)\n",
        "\n",
        "        is_transformer = model_selected in [3]\n",
        "\n",
        "        # Initialize decoder input with <SOS> token\n",
        "        decoder_input = torch.full(\n",
        "            (batch_size, 1) if is_transformer else (batch_size,),\n",
        "            sos_token_idx,\n",
        "            dtype=torch.long,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        decoder_hidden = None\n",
        "        decoded_sequences = [[] for _ in range(batch_size)]\n",
        "        finished = [False] * batch_size\n",
        "        max_length = labels.size(1) + 10  # Safety margin\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            if is_transformer:\n",
        "                tgt_mask = generate_square_subsequent_mask(decoder_input.size(1), device=device)\n",
        "                decoder_output = decoder(decoder_input, encoder_outputs, tgt_mask=tgt_mask)\n",
        "            else:\n",
        "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            predicted_token = topi.view(-1)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                if not finished[i]:\n",
        "                    token_id = predicted_token[i].item()\n",
        "                    decoded_sequences[i].append(token_id)\n",
        "                    if token_id == eos_token_idx:\n",
        "                        finished[i] = True\n",
        "\n",
        "            if all(finished):\n",
        "                break\n",
        "\n",
        "            decoder_input = predicted_token.detach().unsqueeze(1) if is_transformer else predicted_token.detach()\n",
        "\n",
        "        decoded_texts = [decode_sequence(seq, inv_vocab) for seq in decoded_sequences]\n",
        "        gt_texts = [decode_ground_truth(lbl, inv_vocab) for lbl in labels]\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i in range(num_samples):\n",
        "        img_np = images[i].cpu().squeeze().numpy()\n",
        "        plt.subplot(1, num_samples, i + 1)\n",
        "        plt.imshow(img_np, cmap='gray')\n",
        "        plt.title(f\"GT: {gt_texts[i]}\\nPred: {decoded_texts[i]}\")\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print metrics\n",
        "    char_acc = character_accuracy(decoded_texts, gt_texts)\n",
        "    word_acc = word_accuracy(decoded_texts, gt_texts)\n",
        "    print(f\"Evaluation on Sample:\")\n",
        "    print(f\"Character-level Accuracy: {char_acc:.4f}\")\n",
        "    print(f\"Word-level Accuracy:      {word_acc:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "# Utility function: Counts total number of trainable parameters in the model for model size evaluation\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# Utility function: Computes Character Error Rate (CER) as total edit distance normalized by total GT characters\n",
        "def character_error_rate(preds, gts):\n",
        "    total_edits, total_chars = 0, 0\n",
        "    for p, g in zip(preds, gts):\n",
        "        total_edits += levenshtein_distance(p, g)\n",
        "        total_chars += len(g)\n",
        "    return total_edits / total_chars if total_chars else 0\n",
        "\n",
        "\n",
        "# Utility function: Computes word-level accuracy allowing up to k character differences (edit distance tolerance)\n",
        "def word_accuracy_at_k(preds, gts, k):\n",
        "    correct = sum(levenshtein_distance(p, g) <= k for p, g in zip(preds, gts))\n",
        "    return correct / len(gts) if gts else 0\n"
      ],
      "metadata": {
        "id": "NUvsels2A-p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mChlcnGJLeVd"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAvTBTSVFuDR"
      },
      "outputs": [],
      "source": [
        "# Load MJSynth dataset (non-streaming)\n",
        "ds = load_dataset(\"priyank-m/MJSynth_text_recognition\", split=\"train\")\n",
        "\n",
        "# Shuffle and select num_images samples\n",
        "sampled = ds.shuffle(seed=42).select(range(num_images))\n",
        "assert len(sampled) == num_images, f\"Sampled {len(sampled)} images, expected {num_images}\"\n",
        "\n",
        "# Define output paths with versioning\n",
        "base_dir = \"/content/drive/MyDrive/mjsynth_sampled\"\n",
        "images_dir = os.path.join(base_dir, \"images\")\n",
        "os.makedirs(images_dir, exist_ok=True)\n",
        "\n",
        "label_txt_path = os.path.join(base_dir, f\"label_{num_images}.txt\")\n",
        "\n",
        "# Save sampled images and label file\n",
        "with open(label_txt_path, \"w\") as f:\n",
        "    for idx, item in enumerate(sampled):\n",
        "        image: Image.Image = item['image']\n",
        "        label = item['label']\n",
        "\n",
        "        filename = f\"img_{idx:05d}.jpg\"\n",
        "        full_img_path = os.path.join(images_dir, filename)\n",
        "        relative_path = os.path.relpath(full_img_path, base_dir)\n",
        "\n",
        "        if not os.path.exists(full_img_path):\n",
        "            image.save(full_img_path)\n",
        "\n",
        "        f.write(f\"{relative_path}\\t{label}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlXy0v1BJZ7_"
      },
      "source": [
        "# Pre - Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2exHiJrkDrMG"
      },
      "outputs": [],
      "source": [
        "class TextRecognitionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Purpose:\n",
        "        Read image paths and corresponding text labels from a text file (label.txt)\n",
        "        Load and transform the image\n",
        "        Encode the text label into a sequence of integers\n",
        "        Provide the image and encoded label to the model during training or evaluation\n",
        "    \"\"\"\n",
        "    def __init__(self, label_file, transform=None, char_list=None, max_label_len=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            label_file: Path to label.txt (format: img_path \\t label)\n",
        "            transform: torchvision transforms to apply to image\n",
        "            char_list: character vocabulary (sorted list of unique characters)\n",
        "            max_label_len: max length for label padding\n",
        "        \"\"\"\n",
        "        self.samples = pd.read_csv(label_file, sep=\"\\t\", names=[\"img_path\", \"label\"])\n",
        "        self.samples.dropna(inplace=True)  # Drop rows where label is NaN\n",
        "\n",
        "        self.transform = transform\n",
        "        self.label_file = label_file\n",
        "\n",
        "        # Build vocabulary if not provided\n",
        "        if char_list is None:\n",
        "            all_text = \"\".join(self.samples[\"label\"])\n",
        "            base_chars = sorted(set(all_text))\n",
        "        else:\n",
        "            base_chars = char_list\n",
        "\n",
        "        # Ensure special tokens are at the start of the vocabulary\n",
        "        special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]\n",
        "        for token in reversed(special_tokens):  # insert reversed to maintain order\n",
        "            if token not in base_chars:\n",
        "                base_chars.insert(0, token)\n",
        "\n",
        "        self.char_list = base_chars\n",
        "\n",
        "        # Set max label length (without special tokens)\n",
        "        self.max_label_len = max_label_len or self.samples[\"label\"].str.len().max()\n",
        "\n",
        "        # Pre-encode labels\n",
        "        self.encoded_labels = self.samples[\"label\"].apply(self.encode_label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.samples.iloc[idx][\"img_path\"]\n",
        "        label = self.encoded_labels.iloc[idx]\n",
        "\n",
        "        image_path = os.path.join(os.path.dirname(self.label_file), img_path)\n",
        "\n",
        "        assert os.path.exists(image_path), f\"{image_path} not found\"\n",
        "\n",
        "        image = Image.open(image_path).convert(\"L\")\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "    def encode_label(self, text):\n",
        "        sos_idx = self.char_list.index(\"<SOS>\")\n",
        "        eos_idx = self.char_list.index(\"<EOS>\")\n",
        "        pad_idx = self.char_list.index(\"<PAD>\")\n",
        "\n",
        "        label = [self.char_list.index(c) for c in text]\n",
        "        label = [sos_idx] + label + [eos_idx]\n",
        "\n",
        "        # Pad to fixed length (max_label_len + 2 for SOS and EOS)\n",
        "        max_len = self.max_label_len + 2\n",
        "        label += [pad_idx] * (max_len - len(label))\n",
        "\n",
        "        return label\n",
        "\n",
        "    def decode_label(self, encoded):\n",
        "        \"\"\"\n",
        "        Decodifica una lista de Ã­ndices en una cadena de texto (excluye tokens especiales).\n",
        "        \"\"\"\n",
        "        pad_idx = self.char_list.index(\"<PAD>\")\n",
        "        sos_idx = self.char_list.index(\"<SOS>\")\n",
        "        eos_idx = self.char_list.index(\"<EOS>\")\n",
        "\n",
        "        chars = []\n",
        "        for idx in encoded:\n",
        "            if idx in (pad_idx, sos_idx):  # Ignora PAD y SOS\n",
        "                continue\n",
        "            if idx == eos_idx:\n",
        "                break\n",
        "            chars.append(self.char_list[idx])\n",
        "        return ''.join(chars)\n",
        "\n",
        "\n",
        "    def get_vocab(self):\n",
        "        return self.char_list\n",
        "\n",
        "    def get_max_label_len(self):\n",
        "        return self.max_label_len\n",
        "\n",
        "    def custom_collate_fn(self, batch):\n",
        "        images, labels = zip(*batch)\n",
        "        images = torch.stack(images, dim=0)\n",
        "        pad_idx = self.char_list.index(\"<PAD>\")\n",
        "        labels = [torch.tensor(lbl, dtype=torch.long) for lbl in labels]\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=pad_idx)\n",
        "        return images, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3kALQsWKJZs"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zkzB1pD3K8vV",
        "outputId": "7c015d24-19d7-4d47-d713-73245b922a3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 32, 128])\n",
            "[1, 15, 27, 33, 26, 32, 17, 30, 32, 17, 26, 27, 30, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def collate_fn_wrapper(dataset):\n",
        "        def collate_fn(batch):\n",
        "            return dataset.custom_collate_fn(batch)\n",
        "        return collate_fn\n",
        "\n",
        "# Define image transformations to ensure same image size #### WITH NORMALIZATION?\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "label_file = f\"/content/drive/MyDrive/mjsynth_sampled/label_{num_images}.txt\"\n",
        "dataset = TextRecognitionDataset(label_file, transform=transform)\n",
        "\n",
        "# Create DataLoader with your custom collate_fn from dataset instance\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn_wrapper(dataset))\n",
        "\n",
        "# === Add special tokens to vocabulary ===\n",
        "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\"]  # Padding, Start-of-Sequence, End-of-Sequence tokens\n",
        "\n",
        "# Merge special tokens with actual vocabulary, making sure to exclude duplicates\n",
        "vocab_list = special_tokens + [c for c in dataset.get_vocab() if c not in special_tokens]\n",
        "\n",
        "# Create mappings from token to index and vice versa\n",
        "token2idx = {token: idx for idx, token in enumerate(vocab_list)}\n",
        "idx2token = {idx: token for token, idx in token2idx.items()}\n",
        "\n",
        "vocab_size = len(token2idx)  # Total vocabulary size\n",
        "\n",
        "# Example sample\n",
        "image, label = dataset[0]\n",
        "print(image.shape)\n",
        "print(label)  # Should print the padded encoded label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90mAqWerLABi"
      },
      "source": [
        "## Example image and its label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "fr-lIZ08LDY8",
        "outputId": "f3fc1116-9bdd-43c8-ab54-32c2f583fb7a"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABOIAAACmCAYAAAB3ACAvAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAATXZJREFUeJzt3Xd0FOX6B/BnU0glhDSSEEjooUlTQUSCtIsIP4pUEVFRL+hFRMWLiEoRaYKilAuKoFSVclVQOlhAEPTSa4BQhSRACBBKyvP7g7PDvM/uzrtLYNX4/ZyTc/bZmZ0+805mZ75rY2YmAAAAAAAAAAAAuKN8/ugJAAAAAAAAAAAA+DvAhTgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL8CFOAAAAAAAAAAAAC/AhTgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL8CFOAAAAAAAAAAAAC/AhTgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL/jDLsSlpaWRzWajd99997YNc/369WSz2Wj9+vW3bZjw5zB06FCy2WyUmZn5R08KAAAAAAAAAMAt8ehC3KxZs8hms9HWrVvv1PT8ZX3zzTeUkpJCMTExFBwcTOXLl6cuXbrQ8uXLHfo9e/YsDRw4kKpUqUKBgYEUERFB//jHP2jp0qVOh52RkUH9+/en5ORkCgoKopiYGLr33nvp3//+N126dKlQ0/3cc8+Rj48PnTt3Tnn/3Llz5OPjQwEBAXT16lWl2+HDh8lms9HgwYPdHk9OTg4NHTq0yF8kbdKkCdlsNqd/ycnJDv3v3r2bHnvsMSpdujQFBARQfHw89ejRg3bv3u10+Dt37qROnTpRYmIiBQYGUunSpalFixb04Ycf3ulZs5SUlKTMa0xMDD3wwAO0ZMkSp/3fe++9ZLPZaOrUqU6724819r/AwECKj4+nf/zjH/TBBx/QxYsX7+TsuMV+cdj+FxwcTNWqVaMhQ4ZQdna2Q/9Tpkwhm81G9evXdzlM8/D8/PwoIiKC6tWrR/3796c9e/bcydlxi/0LFPufr68vlS1bljp06EDbtm1z6D8rK4sCAwPJZrPR3r17nQ7ziSeeUIYZGhpK5cuXp06dOtGiRYuooKDgDs+VntyvIyIi6J577qFPPvnE6fR16dKFbDYb/fvf/3Y6PPuXRva/gIAAKlWqFDVp0oTeeecdysjIuNOzpOVsH6xcuTL961//ojNnzjj0/+2335LNZqP4+HiX68x8nPDx8aHw8HCqWbMmPfvss7R58+Y7PUtuMc+zj48PxcfHU8uWLZ22Xfn5+RQfH082m42+++47p8NzdpwoW7YstW3blmbOnEnXrl27w3OkJ/fBsLAwqlWrFo0fP97p9L366qtks9moa9euTocnjxP+/v4UFRVFDRs2pMGDB9OxY8fu9CxpyX3Q39+fypcvT48//jgdPnzYof+9e/ca+0FWVpbTYZqPEz4+PhQWFkZVqlShnj170qpVq+7wHLkHbTXaarTVN6GtRltth7YabfUf0Vb7eXVsRdS7775LAwcOpJSUFHrttdcoODiYUlNTafXq1bRgwQJq1aqV0e/+/fupWbNmlJGRQU8++STdfffdlJWVRXPnzqW2bdvSK6+8QuPGjTP6P3fuHN19992UnZ1NTz31FCUnJ9PZs2dpx44dNHXqVOrbty+Fhobe8rQ3atSIpk6dShs2bKC2bdsa72/cuJF8fHwoNzeXtm7dSo0aNTK6bdiwwfisu3JycmjYsGFEdGMHKMoSEhJo1KhRDu+XKFFCqRcvXkzdu3eniIgI6t27N5UrV47S0tJoxowZtHDhQlqwYAF16NDB6H/jxo304IMPUtmyZemZZ56h2NhYOn78OG3atIkmTpxI/fr1u+PzZqV27dr08ssvExHRqVOnaNq0adSxY0eaOnUq9enTx+jv4MGDtGXLFkpKSqK5c+dS3759XQ5z+PDhVK5cOcrNzaXTp0/T+vXr6cUXX6QJEybQ119/TXfdddcdny+dqVOnUmhoKF26dIlWrlxJI0eOpLVr19KGDRvIZrMZ/c2dO5eSkpLol19+odTUVKpYsaLT4bVo0YIef/xxYma6cOECbd++nT799FOaMmUKjRkzhl566SVvzZpL3bt3p9atW1N+fj7t3buXpk6dSt999x1t2rSJateubfT35Zdfks1mo9jYWJo7dy69/fbbTocXEBBAH3/8MRERXblyhY4ePUrffPMNderUiZo0aUJfffUVhYWFeWPWXDLv1xkZGfTZZ59R79696cCBAzR69Gijv+zsbPrmm28oKSmJ5s+fT6NHj1a2A7MXXniB7rnnHsrPz6eMjAzauHEjvfXWWzRhwgT64osvqGnTpl6ZNyv2ffDq1av0008/0dSpU+nbb7+lXbt2UXBwsNGffftOS0ujtWvXUvPmzZ0Oz3ycuHjxIu3du5e+/PJL+uijj2jAgAE0YcIEr8yXFfM+eOTIEZoyZQo1bdqUli1bRg899JDR39q1a+n33383jmXmbpL9OHHt2jU6efIkrVixgp566il6//33aenSpVSmTBlvzJpL5n0wKyuLFi1aRK+88gpt2bKFFixYYPTHzDR//nxKSkqib775hi5evEjFixd3Okz7caKgoIDOnz9PW7Zsoffff58mTpxIM2bMoG7dunll3qzY98Hc3Fz67bffaPr06bRs2TLauXMnxcfHG/3NmTOHYmNj6fz587Rw4UJ6+umnnQ7PfJy4fPkypaam0uLFi2nOnDnUpUsXmjNnDvn7+3tl3lxBW422Gm012mq01c6hrUZb7dW2mj0wc+ZMJiLesmWLJx9z6siRI0xEPG7cuEIPy27dunVMRLxu3brbNkxntm/fbrzOzc3lsLAwbtGihdN+z5w5Y7y+fv0616hRg4ODg3nTpk1Kf3l5edy1a1cmIl6wYIHx/tixY5mIeMOGDQ7DvnDhAl+5csWoU1NT+fLlyx7Ny9GjR5mI+NVXX1XeHzRoENepU4eTk5N51KhRSrdnn32WfXx8+Pz5826PJyMjg4mI33rrLY+mz+6tt95iIuKMjIxb+ry7PFl+BQUFvGPHDuW9lJQUrl69uvazqampHBwczMnJyZyenq50y8jI4OTkZA4JCeFDhw4Z77du3Zqjo6OdLnfzdsbMvGvXLs7Pz3d7XjyRl5fHu3fvVt5LTEzkhx9+WHnv999/55CQEK5cubLy/ptvvskxMTG8aNEittlsfOTIEYdxWB1r1qxZw0FBQZyYmMg5OTnG+8ePH+dz584VYs6s7d27l69fv27UrrbJjh07MhHxxo0bjfcOHz7MRMSLFy/m6OhoHjp0qNNxEBE///zzDu9nZmbyfffdx0TEy5YtU7qZj0e3W3Z2trJ+XB23v/76ayYifvbZZ5X3GzduzB07duQBAwZwuXLlnI6jV69eHBIS4rTbqFGjmIi4S5cuyvtyXdxO7u7Xly9f5oSEBA4JCVGm5ZNPPmF/f39eu3YtExGvX7/eYRz2turLL7906LZt2zaOiYnh8PBwPnXqlPF+RkaGUt9uR48eVY4trvbBl156iYmI582bZ7x36dIlDgkJ4Q8++IDr1KnDTzzxhNNxODtOMDPn5ORw+/btmYh4ypQpSje5Lm6nK1eu8P79+5X3nO2DO3bsYCLili1bKu8//vjjXLduXZ44cSKHhITwpUuXHMZh1XbNmTOHfXx8uH79+sr7t9KWe0IeM5ztg/n5+Xz33XczEfHJkyeN9+3b9dq1a9nf359nzZrlMHyr87u0tDSuXLkyFytWjLdt22a8f+nSJaW9u93S09OV/cfVPvjBBx8wEfE777xjvFdQUMBJSUn80ksvcYcOHbhJkyZOx+Gq/c/Ly+PnnnvO6bkW2urbD2012moztNVoq9FWo602+zO11bc9I+769ev05ptvUr169ahEiRIUEhJCDzzwAK1bt87lZ9577z1KTEykoKAgSklJoV27djn0s2/fPurUqRNFRERQYGAg3X333fT1119rpycnJ4f27dtX6Gyxc+fO0Ycffki1atWixo0bG+9nZmZSdnY23X///U4/FxMTY7xetGgR7dq1iwYNGuRwy7uvry9NmzaNwsPDaejQocb7hw4dIl9fX2rQoIHDsMPCwigwMNCoZ8+eTXFxcdSnTx/asmWLW/NVtmxZKlOmjHGXm92GDRvo/vvvp4YNGzrtVr16dQoPD3drfaelpVF0dDQREQ0bNsy4HdQ8n/v27aMuXbpQdHQ0BQUFUZUqVej11193mN6srCx64oknKDw8nEqUKEFPPvkk5eTkOPQ3Z84cqlevHgUFBVFERAR169aNjh8/rvTTpEkTqlGjBv3666/UuHFjCg4Odutx21OnTtE777xDlSpVop49e2r7d2bcuHGUk5ND06dPN5aNXVRUFE2bNo0uX75MY8eONd4/dOiQsdwl83ZGRPT8889TuXLlaOjQobft1uLU1FQaPHgwlSlTxq3lFBsbS1WrVqUjR44o78+bN486depEbdq0oRIlStC8efM8mo6mTZvSG2+8QUePHqU5c+YY769evdp4tHfdunXEzB4N15nLly/TzJkzqVGjRlS1alW6fPmyW9NHRMp8z507l0qWLEkPP/wwderUiebOnevRdERGRtKCBQvIz8+PRo4cqXSrVasW3XvvvTRt2jSnj9ncip9++omefPJJiouLo//+97/a/p3N87Fjx+jHH3+kbt26Ubdu3ejIkSO0ceNGj6Zj0KBB1LJlS/ryyy/pwIEDxvujR4+m0qVL0yuvvOLyMRpPebpfBwcHU4MGDejy5cvK4ylz586lFi1a0IMPPkhVq1b1eF3XqlWL3n//fcrKyqJJkyYZ7+/atYvKli1L7dq1o6+//pry8vI8Gq4z169fp4ULF1KrVq2MO3J1nK3rJUuW0JUrV6hz587UrVs3Wrx4sUOkgZWgoCCaPXs2RURE0MiRI5V9t23btlStWjUaP348paenuz9zFrZv3079+vWj+Ph4mjJlirb/mjVrUlRUlDLPV65coSVLllC3bt2oS5cudOXKFfrqq688mo4ePXrQ008/TZs3b1Yeh7iVtlzH1TmMKz4+Psbd6+btYu7cuVStWjV68MEHqXnz5h5v34mJiTRr1iy6fv260r5lZGRQxYoVqWnTpjRv3jyPth9XCgoKaPny5dS5c2dKSEhw65EqZ9v3hg0bKC0tzTiW/fDDD3TixAm3p8PX15c++OADqlatGk2aNIkuXLhgdENbjba6MNBWo612BW31DWir0Va7y9ttNRHd/jviMjIyOC4ujl966SWeOnUqjx07lqtUqcL+/v78v//9z+jPfhW2Zs2anJSUxGPGjOFhw4ZxREQER0dH8+nTp41+d+3axSVKlOBq1arxmDFjeNKkSdy4cWO22Wy8ePFioz9nd8TZ37uVO7EKCgp41apV3K1bNw4ICGCbzcYpKSk8Z84co5/8/HwOCgrievXq8dmzZy2H9+ijjzIRcVpamst+evXqxUTEBw8eZGbmd955h4nI6ZVsaf/+/dy3b18ODw83lu3777/PmZmZlp/r3r07BwQE8NWrV5mZ+dq1axwYGMjz5s3jjz/+mCMiIrigoICZmc+dO8c2m4379u3LzO6t70uXLvHUqVOZiLhDhw48e/Zsnj17tnGlf/v27RwWFsaRkZH82muv8bRp0/jVV1/lmjVrGtNo/6aiTp063LFjR54yZQo//fTTTq9av/3222yz2bhr1648ZcoUHjZsGEdFRXFSUpLyLVJKSgrHxsZydHQ09+vXj6dNm8b//e9/nS6j3NxcXrJkCbdp04Z9fX3Z19eXH374YYdvPFNSUjg5OZkzMjIc/szfwMTHx3NSUpLleklKSuKEhASjbtmyJRcvXpx37txp+TnmG99Et2/fnv39/dnHx4dbtmzJn3/+OV+7dk37WbOcnByePXs2p6SkMBFxQEAAd+3a1eEOTWffnl2/fp1LlSrFsbGxxnubNm1iIuIff/yRmZmfeuoprlatmsN4dcea48ePMxFxp06djPdOnjzJAwcO5NjYWCYiLl++PL/99tt84sQJj+bZPp3PPPMMFy9enImI69Wrx5MmTeK8vDyjH1ffng0YMICJiJcvX268l5yczL1792Zm5h9++IGJiH/55ReH8ZKLb9ntmjVrxj4+PnzhwgXjvenTp3ODBg2YiDg4OJh79erFP/zwg8fzfPr0aWP/JSKOiori/v37u/Ut+/bt25mIuFu3bsZ7o0eP5tDQUONOiAoVKvBzzz3nMF6rb9mZmWfPns1ExJMmTTLe27p1K/fs2ZODg4OZiLhhw4Y8Y8YMvnjxokfz7Ml+7ezbs7p167Kvr6/xjejJkyfZx8eHZ8+ezczMw4cP55IlSzrsd1bfsjPf2HeCgoL47rvvNt47f/48Dxs2jMuVK8dExHFxcTxo0CA+cOCAR/PMfKNNHTBgAEdFRTERcZUqVXj06NHKMcrVPjhx4kQmIv7Pf/5jvNeqVStu1qwZM9/4tt5ms/EXX3zhMF5X37Lb9e7dm4mId+3aZby3cOFCbt68Ofv4+LC/vz937NiRv/32W2VfdEdWVhZPmTKF69Wrx0TExYsX5969ezscT53tg+fOnWNfX19u0KCB8d6CBQvYZrPxsWPHmJm5adOm3Lp1a4fx6u7m/vHHH5mI+JVXXjHeu9W2XHLnHIbZ9T7YoUMHJiLet28fMzNfvXqVw8PDecSIEczM/Nlnn7Gvry///vvvyufceeKhQoUKHB0dbdRXr17l8ePHc40aNZiIODw8nJ9//nn+7bffPJpn+/jfeOMNLlOmDBMRlylThocMGaLcfe5qH/zqq6+YiHjQoEHGe3369OEKFSow8402MTQ0lMeOHeswXt0d8SNGjGAi4qVLlxrvoa1GW+0ptNVoq9FWo602Q1t9w1+hrZZu+4W4vLw8h4k6f/48lypVip966injPfvKDwoKUhrfzZs3MxHxgAEDjPeaNWvGNWvWNC4UMd/YaBs2bMiVKlUy3rtdF+KOHTvGw4cP56SkJGXDSE1Nddr/m2++yUTEISEh/NBDD/HIkSP5119/deivdu3aXKJECctxT5gwgYmIv/76a2a+0eBGR0czEXFycjL36dOH582bx1lZWS6HceXKFZ47dy43a9aMbTabcUK2cuVKp7dVTp48WTnh+vnnn5mI+OjRo7xnzx4mIuPxhqVLlzIR8dy5c5nZ/fVt9Whq48aNuXjx4nz06FHlffvFP+abB0jzMJlvHHwiIyONOi0tjX19fXnkyJFKfzt37mQ/Pz/lffsJq7mRkvbt28cDBw7kUqVKKY2gq9vO7cN09vfPf/6TmW80MkTE7dq1czleZub/+7//YyLi7OxsZmZeuXKlcQJy33338auvvsorVqywvO0/PT1dOWBGRkbyiy++qL2Yt3XrVu7bty+XKFFCObl19ThJYmIit2zZ0rjouH37du7WrRsTEffr18/o71//+heXKVPGWLcrV65kIlIu0jO7d6wpUaIE16lTx+H93Nxc/uqrr4wDpq+vL7du3ZoXL15suawyMjJ4woQJXL16dePk9sUXX3T5OIl9m9y/fz9nZGTwkSNHeNq0aRwQEMClSpUyTvi2bt3KRMSrVq1i5hvbdUJCAvfv399hmLqT+/79+zMROZ2mPXv28CuvvGJsq5UrV+bRo0c7NLxmeXl5/M0333D79u3Zz8/POLlduHCh02VlP24PGzaMMzIy+PTp07x+/XquU6cOExEvWrTI6LdmzZrco0cPox48eDBHRUVxbm6uMkzdyf3//vc/hzbB7sKFCzxt2jSuX78+ExGHhoZy7969lUeNnLmV/dp8gX3v3r38wgsvMBFx27Ztjf7effddDgoKMvbZAwcOMBHxkiVLlOHpTu6ZmWvVqsUlS5Z0eL+goIDXrl3Ljz32GAcFBTERcePGjfnTTz9VHv+SsrOz+aOPPjKWlf3k1lnsAfPNfXD16tWckZHBx48f5wULFnBkZKTSbp85c4b9/Pz4o48+Mj7bsGFDp8c33cn9e++9x0TEX331lUO3o0ePKv/cJCQk8JAhQ/jw4cMuh1dQUMDr16/nnj17clBQkHFyO2vWLJePkxAR9+7dmzMyMjg9PZ03b97MzZo1YyLi8ePHG/21adOG77//fqOePn06+/n5OUQN6E7uz58/b3xJJXnaltt5eg5j3wft23dqaiq/8847bLPZ+K677jL6W7hwofJFYXZ2NgcGBvJ7772nDM+dk/t27doxESkXKux++eUX7tOnj/HPTZ06dXjy5MmWcRhXr17l+fPnc/PmzZVltWLFCqfLyr4PfvLJJ8ajZMuWLeOkpCS22WxG23P9+nWOjIzk119/3fjso48+yrVq1XIYpu7kfsmSJUxEPHHiRIduaKvRVqOtdoS2Gm012uqb0Fb/ddtqV+5oRlx+fj6fPXuWMzIy+OGHH+batWsb3ewrv3v37g6fq1+/PlepUoWZmc+ePcs2m41HjBjhcIfRsGHDmIiMg0xhM+I2b97MrVq1Yh8fH+2GIc2bN48bNWrEPj4+xoWXOnXq8J49e4x+KlSooNzh5MxHH33ERKRcBT916hT36dPHaIiIiIsVK8bDhw9XLlY5k5aWxkOHDjV28sTEROV5auab35DZs+DeffddLl26NDPfODhGRETw9OnTmZn5tddeMy7SSVbr29WFuPT0dCYipyc6ZvYDpPxm0n7h0n6AmDBhAttsNj548KDD9lK1alVu3ry58dmUlBQOCAhwejV7+fLl3KhRI7caQbOUlBROSkriVatWOfzt3buXmW9+Q/zYY49ZDqtHjx7K9s1846DXoUMH49tFIuLo6GinjaG0efNm5YB57733KneUMjPPnTuXa9eu7dbJrVliYqLDhUdfX1/u2bOnccKRm5vL0dHRyrdJeXl5HBMTo7zH7N6xpnTp0lyxYkXL6Tpz5gy/++67xgm7fVzmE9f9+/dz586duVixYtqTWzP7Nin/qlevzlu3bjX6GzBgAJcqVUr5VvDll192eI9Zf3L/+uuvMxHxTz/95LIf+z837dq1Y39/f/bz8+N27do5rMfBgwdzfHy8Wye3dvbjtvwLCwvjMWPGGP3Zjynmb5N27tzp8B6z/uT+4MGDTET89NNPW06b/OemWrVqygknc+H2aznPNpuNH374YeWkrU6dOsqdH8zM9erVc3jPnZP7+++/n/38/Cyn68KFC/yf//zHOGEvUaIE9+nTR/kn/Pfff+cnn3ySQ0JC3Dq5tbPvg/IvMTFRuYNk4sSJXKxYMWWcH374ocN7zPqTe2ftn1RQUMBr1qzhHj16GCfszZo14++//17p7/333+eKFSu6dXJr5myeAwMD+aWXXjLOBTIzM9nf31+58+Ps2bMO7zHrT+5zc3OZiJS2yRl32vJbPYex34kv/xo2bKhkwXTo0EG584OZ+ZFHHnF4z52Te2ftmyT/uQkMDOQePXoo5x+XLl3iF154gSMiIty6EGVn3wflX3R0NH/22WdGf/Zv3c13fnzzzTcO7zHrT+5XrVrFRMRvv/225bShrUZbbYa2Gm21FbTVaKvRVv9522qdO3IhbtasWVyzZk329/dXFpo5ANS+8t98802Hz/fs2ZMDAgKY+eYdclZ/9tshC3shzr4TRkdHOzRA7rpw4QKvXLnSeAy1QoUKxg8qeHJHnLMLKwUFBbx//37+4IMPuHTp0kxEDo2XK6dPn+a2bdsaDYBZfn4+h4eHc5s2bZj5xg5sDlx9+OGHuVevXsx84+61MmXKKJ93Z327uhBnf/xBNx/2dWN+ZJn55jZpf9y3b9++ltuK+RuDlJQULl++vNPx2Q925cuX559//tly2szc+bEGT++Ic/YtxLVr1/iXX37h1157jQMDA9nf398hlNmVX3/9lZOTk51Og/0kpm7dusaFQ3ckJiZy/fr1edWqVbx69WreuHGjwzciy5YtYyLiL774gg8ePGj8de3alUuXLq00QIX5lt2ZCxcuGI8yE5HToNuQkBCeNWuW22Gc9m1y0aJFvGrVKl6/fr3DyUNeXh7HxcVxt27dlHn+4osvmIh4xYoVSv+6k3urb9mlgoICXrBgAYeFhTEROXwTZl8WPXr0cDs4237cfvbZZ3nVqlW8Zs0a/vXXX5W7lZmZBw4cyCEhIbxnzx5lvpOSkhy+fCnMt+zOpKamGmHZ8puwwuzX9gvsq1ev5p9++snhB1Lsdw+/9957yjy//PLLHBgYqOzHhfmW3ZkrV67wkCFD2GazMZF614p9XH5+fjxu3Di3g7Pt+8XkyZN51apVvG7dOt6zZ4/D/nHPPfdwo0aNlHn+6aefmIh42rRpSr+F+ZbdmdWrVxv/oMovc+wXHFq0aOHRI2/246J9XW/evNkh2Nl+F/mGDRuU+W7UqBHfd999Sr+F+ZbdGau2/FbPYXr16sWBgYHGl0Y//PADHz9+3GE6AwIC+OWXX1bm2X7OYg7SLuy37FJubi5/8MEHXKxYMSZS71oxX3B45ZVX3A7Otu8Xb775Jq9atYrXrl3LO3bscLgLqHPnzlyuXDllnvfs2cPBwcH82muvKf0W5lt2Z9BWo622jwtttXvQVqOtNkNbjbb6j26rdW77hTh7RkD79u35s88+4+XLl/OqVau4adOmnJiYaPTn7oU4+2OSr7zyitO7jFatWmXcWlzYC3HHjx/n119/3bjIZf/myfwrJJ6wNyT2X+Lp3r07Ezm/m8zuiSeeYKKbt5O6cujQIfbx8XH5iyDMjrf9+/n5cfv27R0yFZiZH3roISMLLiYmht9//32j28iRI7lChQp87do1DgoKUhpnd9f37boQJw+Q9m3Sno3xz3/+k202mzEd8s/coFvtiPv27bulK/fu/mpqXFycy1+msktKSjLuTLRiXwauft2L+eZjAfZ8khIlSnDfvn0dfuVoy5Yt/NRTT3FoaCjbbDZu0qQJf/rpp05/YchM12gz38xIdPW3du1ah3nS5c507tzZ5fjkYwH2b+Pmz5+vnKBkZmbyqFGjuFKlSkzk/jdy7vySr/1xHld/jz/+uNK/7uS+WbNm7OvraxzznElLS1MeC0hKSuJhw4Y5XMRetGgRP/TQQ+zj48OBgYHcrVs37Tdy7jTa9sd5XM1zcHCwkg/jbu7M5MmTXfZj/zbOfqt7UFAQP/bYYw7foN/J/Xrw4MGW6/qTTz4x+nU3d+aee+6xHKd8LKB+/fo8ffp05Z+tS5cu8aRJk4xHkty9e8ad9t7+OI+rv8aNGyv9u5s7Y/WlwpkzZ5THAkqVKsUDBw50eOxlzZo13KVLF+PumTZt2vCiRYu0eR66fZD5xuM8VvNt/mba3dyZgQMHuhyfu235rZ7D6PZB5huP81jNs/lczt3cmZiYGMtx7tmzR8kSq169Ok+YMEGJ5sjNzeVZs2bxAw88wEQ37p55+umntXfPuPMP9oULFzgwMNDlPCcmJipPJbibO+PsHMw8TrTVaKvN0FajrbaCtto1tNVoq//otlrntl+Ia9euHZcvX97hkcmGDRs6vRCnezT1zJkzTEQOVzOdKeyFOLu8vDxeunSpksXQunVr/vLLLz0K5fvwww+ZiHj+/PnMfONRAiIywhOlCxcucMmSJTk5Odmt4UdGRhrLyWz37t1O8ydk4242cuRIJiL+73//67CO7cvVfoXY3Mi6u74zMzOZqPCPpuouxI0dO5aJyOFnrp1xp8F29ix7ly5dePny5U5Pgty9EPfMM88w0c1cPskeEmzPlbNif4xA9mu/LdzTfApm5osXLzrNqHD1mIWu0bb/ZHrXrl35yy+/dPiLi4szApKZ9cca+4+YfPzxxw7djhw5otwWbs+nMAcZu7J+/XqH5TVz5kyPf+rcrlevXhwTE+N0nrt3787FixdX1oXVicXRo0fZz8+PGzVq5NAtJyfHZT6F7vH148eP8/Dhw5U8j9dff93plwHuNNr248Xw4cMd5tl+gmAPSLYvI6sTi5YtW7LNZnMadGy/Ldyej+ROPgXz7d+v7T+Z/uCDDzpd13fddZcRkGxeRq5OLObNm8dExEOGDHHoJh/h8iSf4tdff1XypOrWrcsffvih0x8acqe9f+utt9jf358XLFjgMM/9+/dnm82mfPFkdZy4ePEiR0REKLlUdreaJcV8o+2ReVL9+/fnbdu2Oe1fd3J/+PBhJiL+17/+5TDPn3/+ORcrVkxp43XHiWeffZaJbuT7SLfSljN7fg7jzsl9SkoK16hRw+n23bx5c+XRQ91xYuPGjUzkPJ4hKyvLaZaUO3fF7N+/X8mTqly5Mo8aNcrpPzfunNzb94GpU6c6zPPbb7/t0IZbHSfy8vK4atWqHBwc7HBnAdpqtNVoq51DW422Gm31TWirnfsrtNWu3PYLcR07duTy5csrB8hNmzaxzWZzeiHO1Y81vPjii8Z7TZo04YiICKeZCM5+WcN8Ie7y5cu8d+9eywbYyunTp3nMmDFcuXJl40Bqzsi4fPmyy7DR1q1bM9HNR2evXbvG1apV45CQEIdlmJ+fb3wLab9wx3xj2Tk7sbAvp//7v/9T5t++QwQFBXHPnj0dnsV35fvvv2ci4vvuu4+Dg4OVWz4vX77Mfn5+xi3k5m9n3F3fOTk5Li+4efJjDboLcampqezr68uPPvqoQwNRUFCg/JKNuxfN7I4cOcJDhgwxvkEsXbq0w8HL3WEeOHCAg4KCuFq1ag6/rnP27FmuVq0aBwcHK9/0rl271umJ2pgxY5iIeMKECcZ7U6ZMMU5uY2Nj+dVXX72lX2xivvGrTS+++CJHRkYyEXGlSpX4008/VfrRndzbvyl19QthzzzzDIeHhxvfDloda9asWcNBQUFcrlw547FvZuYdO3YYJ7f+/v7coUMHXrZsmce/2MR8o4GZPHmy8c1kaGgoP/nkk0rDqGu0c3JyuHjx4g4/MGK3YcMGJiJesGCB8Z6rE4uzZ89yw4YN2Waz8Xfffad0++c//2mcrNWoUYPfe+89j3+xifnG/rFy5Uru2rUrBwQEMBHxAw88wJs3bzb6cefkvnfv3hwSEqKsG7NKlSpxq1atjNrqxGLUqFFMpP7CG/ONIFr7yVp4eDg/99xzt/SLTcy3Z7+2f1NqzqswGzlyJPv4+BgnGVYnFtu2beOYmBguWbKkEt597NgxbteuHfv5+bHNZuPmzZvzggULbukXm3JycvjTTz/lxo0bM9GNX1js3Lmz0p66095XrFiRmzZt6rTbiRMn2Gaz8ejRo433XB0ncnJyuH379kzk+OM5b731lnGyVq5cOR4xYsQt/boi84077Hv37s2hoaHGP4Pffvut0o/u5N7+Tan9F9ikFi1aKF+mWR0n5s6dyz4+Pg6PyBSmLZd05zDM+pP7Y8eOsc1m4+HDhzvtbv+ScdOmTcxsfZxIS0vjypUrc7FixZRziezsbCNLiIi4QYMG/PHHH3v864rMN/4ZXLx4Mbdu3dr4gaOHHnpIaQPdOblv1qyZy/iKq1evcmhoKPfp08d4z9VxIi8vj5977jkmUn/hjRltNdpqtNXuQluNttoMbbUjtNWO/oxttdktXYjr27cvjxgxwuEvOzubP/nkE+MC0bRp03jQoEEcHh7O1atXd3ohrmbNmpyUlMRjxozh4cOHc0REBEdGRioX3Xbv3s0lS5bkyMhIHjRoEE+fPp1HjBjBrVu3VjK/nF2Is7/nya+muvL999/z448/zvHx8cZ79kcuGzRowEOHDuUZM2bw+PHjjVsv27dvrwxjz549HBcXxwEBAdynTx/++OOP+d133+W6desyEfHLL7+s9P/8889zeHg4P/nkkzxp0iSePn06DxgwgEuUKMGBgYHGzsTMPHToUK5bty5PmTLF8ldVnbly5YrxTLezx13tPyMdHh6uXHRzd30zM1erVo1jY2N58uTJPH/+fONboW3btnFoaChHRkbya6+9xtOnT+fBgwcrmRHuXohjvnlC0LBhQx47dixPnTqVX331Va5UqZJysPH0QpxdXl4eL1u2jDt06OBwO3pKSgonJCTw7Nmznf6ZffHFF+zv789xcXE8ZMgQnjFjBr/xxhscHx/PxYoVU37Vipm5evXqXK5cOX7ppZd4+vTpPGnSJH700UfZ19eXk5KSlG8WmzVrxg8//DAvWbLE4Tn6W3Xt2jVesGABt2jRgjt27Kh0053ct2rViiMjI12eaNsDNe3zbF+vw4cP59mzZ/PMmTN59OjRxjeuSUlJDt8qzpw5k6tUqcJjx451yAQpjN9++42fe+45Dg8PV5ax7uR+wYIFTHTjLlNn8vPzOTo6WvklL6IbORmzZ8/mzz77jCdNmmT84+Pn5+eQHcPMxu3d5mNBYWVmZvJ7771n/LNgpzu5t/9kujzumb388svs5+dnrKNevXpxQECAsY989NFHPGTIEL7rrruYiPjBBx90eLznySef5JSUFP7ss89u+VsoSbdfWx0r+vTpw76+vk6/rWa+edeq/Ze87O3SCy+8wLNnz+ZZs2bx+PHjuWPHjuzn58eRkZFGpIHdunXruEyZMvzGG2+4dceIuw4cOMD//ve/OTY2Vsmr0Z3c22MFzDEGUr169bhmzZpGnZiYyLVr1zbW9dSpU7lfv37G4wyy/WNmrlKlCnfr1o1XrVqlvWPEXRcvXuSPP/6YGzRooHzpx6w/uU9OTlZ+iEiy3wlv/+V0+3Fi6tSpPHv2bJ4xYwYPHz6c77//fia6kY0k/1kpTFtuxdk5DLP+5H706NFMRC7vTDh//jz7+fkZv7ppfuJh9uzZ/Omnn/LEiRONb5GDg4P5888/V4Zx5MgRjoqK4gEDBjgEKxfGiRMneMSIEVyuXDklr0Z3cn/y5En28fFx2D7MHnnkEY6MjDTu9JDt/7Rp03jgwIFcoUIF4yKFbI/RVqOtLgy01Wir0VY7h7baEdrqP1dbzXyLF+Jc/R0/fpwLCgr4nXfe4cTERA4ICOA6derw0qVLuVevXk4vxI0bN47Hjx/PZcqU4YCAAH7ggQecPg9/6NAhfvzxxzk2Npb9/f25dOnS3KZNG164cKHRz52+EGdnvkMtNzeXP/roI27fvr0xz8HBwVynTh0eN26c029A0tPT+aWXXuKKFStyQEAAh4eHc/Pmzfnrr7926HfHjh08cOBArlu3LkdERLCfnx/HxcVx586dHb5V0mWD6NjveBs8eLBDN/vPfz/00EPK++6ub+Ybt7jWq1fPuOBnXie7du3iDh06cHh4OAcGBnKVKlX4jTfeMLp7ciGO+UamRqNGjTgkJIRDQkI4OTmZn3/+eeWR1Vu9EGcml7mzX2wy/0k7duzg7t27c1xcHPv7+3NsbCx3797d6a3r3333HT/11FOcnJzMoaGhXKxYMa5YsSL369fP4WS2sNuCjhy+1cm9/SfTe/bs6XJ4OTk5HBwcbISgymNNsWLFODY2llu0aMETJ050mrtyp+f5ypUrykVo3cl927ZtOTAw0DKQ9IknnmB/f3/jW3HzPPv4+HB4eDjXqVOH+/fv7zKLw5vrWndyv2jRIiYinjFjhsvhrV+/noluhqDKX4EKDg7mpKQkfuSRR3jhwoVOHz/x9vZtdayw/2T6Aw88YDnMcuXKGYHl8leg/P39OTo6mhs3bswjR45Uvu22y8nJcTuc/Fbk5uYqeTW6k/t+/foxkZqxIg0dOpSJbt5Fbf7FRpvNxmFhYVy9enV+5plnlLs5zLy9rq1O7n/99VcmIqVtktLS0pjoZmC5/MXGwMBATkhI4DZt2vAnn3ziEJ7ubJpuNzl83cl9zZo1uWzZspbDbNKkCcfExHBubq7DLzb6+flxREQE169fn1977TWnObnXr1+/pTtG3FVQUKAci3Un9+PHj2ci4jVr1rgc5qxZs5joZmC5bP9DQ0O5UqVK/Nhjj/HKlSudDgNt9e2HthpttTNoq9FWm6GtRlv9Z2irbczMBAAAAAAAAAAAAHeUzx89AQAAAAAAAAAAAH8HuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AW4EAcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AW4EAcAAAAAAAAAAOAFfn/0BAAAwJ1Tq1Ytpc7Ly1Pqq1evKrWf381mwcfHx7LfgoICpQ4LC7PsfunSJZfjIiIKDAx0e1yhoaFKLaf1+vXrlp+XtXla5HTJWo5LDksuY11t/rwcF8AfRW7nfxdyf77d/Zt5uox1/RdmWsD7dOtz7969XpoSAADwtr/nWRYAAAAAAAAAAICX4UIcAAAAAAAAAACAF+AZGACAIkz3iKd8TNLqEU3d453yMZtixYpZTpvs31zLz8r5SE9PtxxWcHCwZW1+DJZInRf5WGtWVpZSy2Um+5fTLsclH+E1L2f5+K4cFwAAAAAA/LXhjjgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL8CFOAAAAAAAAAAAAC9ARhwAQBGmyxiT2WrmPDOZq5adna3UOTk5Si2z0WQtM+Vktpp5Wq3y44iIQkNDyYrMt7Mal7NpsxqWrGUGnOwup11Oi7m2mg4AAAAAAPjrwx1xAAAAAAAAAAAAXoALcQAAAAAAAAAAAF6AC3EAAAAAAAAAAABegIw4AIAi7OrVq0ptlQlHRBQeHm68jomJUbpdunTJctgy30zWMjvNKr8uNjZWqcPCwjyaFjlsWcuctsJkxOmy8OS4raZVDkuuLwAAAAAA+GvDGT4AAAAAAAAAAIAX4EIcAAAAAAAAAACAF+BCHAAAAAAAAAAAgBcgIw4AoAiTeWQyE07mnUVGRhqvK1asqHTLyspSapmzpstOk+MODg5WanNWWsOGDZVuJUuWVOoVK1YodXZ2tuWw5bhDQ0OV2ryc5DLT5c/J+Za5brqMOVkDAAAAAEDRhTviAAAAAAAAAAAAvAAX4gAAAAAAAAAAALwAF+IAAAAAAAAAAAC8AME0AABFWFhYmFLL3DZZmzPiypQpo3RLTU1V6lOnTil1QUGBUssMOUl2N2erJSYmKt0SEhKU+ueff1bqzMxMl8NyNm2yNpOZbTJPTrcM5bglOd/m2pPpBAAAAACAvx7cEQcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBXg0FQCgCJOPphbm0ce8vDylvnr1qmV3OWz5yKf8vLl7YGCg0i0+Pl6pY2JilDo9Pd2jabV6LFY+ahoREaHUcpnKWg770qVLltNq7u7pY64AAAAAAPDXgjN8AAAAAAAAAAAAL8CFOAAAAAAAAAAAAC/AhTgAAAAAAAAAAAAvQEYcAEARJrPSZH5ZTk6OUp89e9Z4nZ2drXSTWWdWOWtEjnlnspbMGXEJCQlKN5kRl5ubazltMo9ON23mPDuZbSfz5eSwZP+6ZSxrc/+66QYAAAAAgL82nOEDAAAAAAAAAAB4AS7EAQAAAAAAAAAAeAEuxAEAAAAAAAAAAHgBMuIAAIowmYWmyxyrWbOm8bpVq1ZKt5MnTyr16dOnldoq+4zIMXOuQYMGSt2iRQvjdY0aNSynU+a2yZy2wMBApZYZcllZWUodHh5uvC5btqzSLSUlRakvXryo1OvXr7ccl05oaKjxWq4vuQzT09NdfpaIKCIiwqNxW20PMl9Qzpes5biDg4OVWm4fVv3qsg1lLde/nC9Zm7P4dBl/un1Gl30oh2feNsPCwpRucruUy1iub5kpKPcLuVzN27ncH+Vn5bTJ+Th16pTltMXGxiq1ef3Lccn1LedLDlsul8zMTKU2zyeR47yYyWmRy0WSxxbd9mGeF10GpFzGctrkuOX6tdov5LjkMpfjknQ5m54cS+R0etpOFRXy2AMAAH8ff4+WDgAAAAAAAAAA4A+GC3EAAAAAAAAAAABegAtxAAAAAAAAAAAAXoCMOACAIkzmuMmMIlmbs3t0mUHyszLnR+YZxcTEKHV+fr5S796923gtM6EkmVcns9NkZpTML6tYsaJSlypVyngdEBCgdPvuu++UWub6REZGKrWnWVrm5aTLykpISFBquY5kxpjM4atVq5ZSm/Pw5LayfPlypZa5W3J9nzt3TqllJpxVbptcZvKzujw6q2XqjHk5y2HrctZ026Yuv85q3HIZyXHJZa7L3pLdZW01brk+Jbk9yG1VZq1Z5WHpjkvys3K5yAw4uQ6t8gnlMpHLWNa67Dyr46Zu25C1LlNOsuquW6bys7r8MqvcRd3nrfYJZ8P6u2TGAQDA3wdaNgAAAAAAAAAAAC/AhTgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL0BGHABAESYzxXTZPOaMsV27dindDh06pNQyh01mRsXGxlpOi8xO27Rpk/E6NzfXctgyQyg+Pl6pZUaY7F/y9/d32a/MiJLZVxkZGUotl4vs3yq/TOZLyWmReWW6ujC5TdeuXVNqmT8n8650OU7y8+b+dTlsMl+wbt26Sl21alWlljlecjs3Zwzu3LlT6SZrSZd9p8vWsqLL3dLNly4zzlzL7doqL9JZ/3KdyG1X5jaah6/LhJNklp4ut80qO1E3brl/ym1TdtflvpmHL7t5Ol+6z1vlMOoy4Tw5VujGJT+v6/fvmgH3d51vAADAHXEAAAAAAAAAAABegQtxAAAAAAAAAAAAXoALcQAAAAAAAAAAAF6AjDgAgCJMZiXJWuaXnThxwngdFRWldLt8+bJSy+yk8uXLK7XMN9JljpnHLfOJZC7T6dOnlVpm7ciMsdq1ayt1zZo1lXrbtm3G64MHDyrdZA6TrOUylTlPlStXVmqZKWXO9ZLLVJchdM899yh1w4YNlfrHH39U6p9//lmpzctVbgsyC+3cuXNKLfuXOX5y/cvu5uUk51tmncllfvjwYaWW60DmE1rleslucruX61OSy0G3PZizuOT6lbmJcr7Lli2r1HLa5fDkcjVPi1V+nLPplsOW24P8vFXeoS6fTtZye5DDltuqVZ6d1f5H5Hjskf3L45acFtm/+Vgk50s33XL96Y49knl8cli6DDhPu1tl0HmaEfd3yU4rTJ4kAAD8tf09WjoAAAAAAAAAAIA/GC7EAQAAAAAAAAAAeAEuxAEAAAAAAAAAAHgBMuIAAIowmSElM4Vk5tCxY8eM1/7+/ko3mWcjM8BkNpI5800OWzc8XY6TzDuS5HwdOHBAqWXOkzlzTmZfybwiXb6VJKfdqrucbpl1JzPC5PrcuXOnUsv5lvOWlJRkvJYZfnJadFlquswxuRzMtVzGctxyfVllGxIRlStXTqmLFy+u1Obllp6ernST+YNyO5frW06bJD9vnrfMzEzLfuUyltu93B7uuusupZbzbd7n5HYs14/clo4fP67Uuu1Frn+rzDBJl52l+7ycN3Mtu+my7XQ5bFbZaJLuWCGHJdeJ/LzcJ2XmnHnedFmHummR5LR5kusm50OXEVdUs9SK6nwBAIAe7ogDAAAAAAAAAADwAlyIAwAAAAAAAAAA8AJciAMAAAAAAAAAAPACZMQBABRhMt9IZvPExMS4/KzMvqpRo4ZSyzyrtLQ0y8/LaZGZReb8K5k/JLN0ZHZalSpVlLphw4ZKvXnzZqVevXo1uSKnS86nzC+SuUyeTru5lhlgCQkJSl2nTh2lPnTokFIvXbpUqWWGlJyXw4cPG69l7prMk7OabiLrfCpn02JeLrocLjndch1FREQodaVKlSyHt2PHDuO1zGmT8y33mfDwcMtazoscvnl4cv3KTDg5X3J/ldvLxYsXlTo3N1epzetMrh+5/uQyTkxMtJwWmQG5bds2pTbPt9yH5D4jl6Gc1vj4eKWuWrWqUu/fv1+pzccmud1KuuwzXbae3N/N2Xm6vEg5nzK/UK5vOS1yuZmnRZfDpptvq8w/d+rb9VmiopOt5kmuHgAAFC1oAQAAAAAAAAAAALwAF+IAAAAAAAAAAAC8ABfiAAAAAAAAAAAAvAAZcQAARZjMfZI5XTKTypwTdPbsWZfdnJHZWnLcMt9I5vyY+5fZOTKPqkmTJkodFxdnOWw532XLllVqT3KcpMLm/JhzwBo3bqx0q1ChglLLfCuZ29eiRQulPn78uFLL3D7zcpHrT+ZPmZcRkWOmnC5jSk67uX85XTKXTW47ctglSpRQ6urVq1tO2549e4zXcluRw5bTIpeL7C6HJ2tz/zJfrm7dukpdrlw5pZY5jL///rtSnzlzRqkvXLig1OblrJtvucxlLpvcJ+UyluvUnPMnh63bdmRGYO3atZX6kUceUepFixYp9enTp12OS9a6bENZy1w2WZv3G/nZihUruuyXyHF967LU5LZp7q7bh+R8y+1DdtetM7lfWA1bp6hkwknIiAMA+PtCCwAAAAAAAAAAAOAFuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AXIiAMAKMJ02Toyx8ecbyQzwDZt2mT5WZlvJMl8JJkhZ65lnpEcl8wn2rdvn1L//PPPSi1zmyTzcpLLTJeFJOdLlxkm867M2WkXL15Uusn5unz5slLLjLHo6GilluswIyNDqZnZeC0zv3r27KnUv/32m1KvWrVKqXV5R3J9mzOn5LYjl5FcxjJ/TGah6daJubvsV257ctpkVpbM5fI0c8wsKCjIst+TJ08qdWpqquW45TK32g88zQyTtS4j0jwvcn3IfmV3c44ikeN2L/MH5fZjrnXZZ7rcNTltutw283yXL19e6dauXTullpl/q1evVmq5nOT6ttqWixcvrnQz7/tEjtuGHLbuGGqVrSmXSVHNfPMUlgMAwN8X7ogDAAAAAAAAAADwAlyIAwAAAAAAAAAA8AI8mgoA8DciH8OSjzqZH5WRj2DJxwEl+ViUfKRL9wiX7N8sMzNTqVeuXKnUcr7kI5m6x+zM47Z6dNAZOSy53OSjjvLxUfP45CO1cj7k41+y1k2LXObm9f/AAw8o3e6++27LaVmxYoXluHTjNtM9Diwfi5PrU26b8jFqf39/pTY/RimnSz7WaPUINZH+kU05/ISEBOP1Pffco3RLS0tT6i1btii1nE/dtMl9KioqyuV0Srr9NT09XanlOpLL0fx53ePecp+Rj6LK5TR9+nSlPnLkiFKbt105brmdWk23s8/rajN5bImNjbWclri4OKU+c+aMUsvHpuXwzes7KSlJ6SbX14kTJ1x+lsjx8V+5TmQdGRlpvJaPLcv1c+zYMaWW25busXc5L+btXk63p8ca2V1um3J45s/L/U/3GDseVQUA+PvAHXEAAAAAAAAAAABegAtxAAAAAAAAAAAAXoALcQAAAAAAAAAAAF6AjDgAgCLMKneNyDEPx0yX+aWjy07S5UKZyewcmY0k6fLqrLLUdNl2cj5kXpkkx8XMSp2fn++yX8nT9anLKzPnOsllJnOd5DKX24MuK032b5XLJ6dTDkvmV8nP7969W6nlcjFnhumWkW671/Uvp61MmTLG6+TkZKXbvn37lPq3336zHJYuh0+XrWXmSdaZs3F7kk8op0vOV0xMjFLXqFFDqWWe2Zo1a1yOS0e3vuS2o8vKtNrH5P6bmpqq1BkZGUp9/vx5pdZlX8bHxyu1eTnKrLRr164ptZxPmfkm97mIiAilLl68uFKbj+cyu1DS5bZ5kstGpObhPfTQQ0q3y5cvK/XZs2eV+vTp00qtW99XrlxRaqs8Qt0+5cl2CwAAf2044gMAAAAAAAAAAHgBLsQBAAAAAAAAAAB4AS7EAQAAAAAAAAAAeAEy4gAA4E9Plz8m6TLhrHLBdJlhulp+XuYjpaenK7U5k0qXpaTLr/M0Y8icWfT7778r3b7//nulPnjwoMvPEukzw6zmTWZA6WqZIRYUFKTUMvdJZmmZl7nMl9KtAx25jmQ2lzkjMDMz02U3Z5+V+WWSzPGzyq/T7UNW+ZFE1pmORI7L3CqHUfYr88hatGih1HIfktuev7+/UoeEhBivo6OjlW65ublKfejQIaU+fvy4Ust1JpdxWFiYUteuXdt4LXPV9u7dq9THjh1TapmFJ8nhPfjgg0pt3k/k/izz6WQ2mpwWue3JbVMuB3PGpO5YIelyOeW2K6fNvMyffvpppdu2bduU+scff1TqgIAAy2HL46Q5j46I6L777jNeb9myRem2adMmAgAAIMIdcQAAAAAAAAAAAF6BC3EAAAAAAAAAAABegAtxAAAAAAAAAAAAXoCMOAAA+MvR5bjpMohk5pC5lt0kOS6ZdyU/L/OvzNlJclrlZ3VZdzq6bDxzLTOhsrOzlVpmp3m6nKzWmS77TrLZbEots9XktMrsNDNd/pRu2jzJHyQiOnPmjMvpktl3Mn/qwoULSp2RkaHUMlNMLgdzDpjMMpNZWLK7pMurkxlz5uUi9wm5DmTGW0JCguW44+Pjlbp48eJKbZ4Xmasmt3OZlSbHJfPr5PYhM+jM+Xay3xkzZliOWy4XuU5iY2OVukGDBkodHh5uvF6zZo3STW578pipy07UHXPN0y67yfmQw9Yda2Q+Xfny5ZXavJx/+uknpZtcDqtXr1Zqub7luLOysiynxbx9NWvWTOlWvXp1pV63bp1S79u3jwAA4O8Bd8QBAAAAAAAAAAB4AS7EAQAAAAAAAAAAeAEuxAEAAAAAAAAAAHgBMuIAAOAvR5dPpMttk5+3yojTZcLJWtJliJnzsuR8eJpH5mmGnJnMjMrMzFRqmZ0UHBzs0bit5sUqT8wZc86aM57k28llLudLl1cnu8v8M5m9Zx6fzJcqU6aMUteuXVup5To6ePCgUqenpyu1Va6X3M7lOrDK/HI2bF22nrm7Lkfv4sWLSv3LL78o9YEDB5T622+/tZx287To5kNmOMr5MOeuEenXYd26dV1Ol8xKk8tB1jKfTGYCyn3WvC3LHD053TKfUC4HuW3JPDs5beZpl/OtO87J7nJaZXZiSkqKUp89e9Z4PWbMGKWbnC+5/iWrtoKIaMeOHUpt3t8HDhyodBs0aJBSy2W6Z88ey2kBAICiA3fEAQAAAAAAAAAAeAEuxAEAAAAAAAAAAHgBLsQBAAAAAAAAAAB4ATLiAADgL0eXpaSrZUaVVUacrHX5RjJTSpcxZCZz2CTdtMn50mUzWeV2SbrcJpmNJmvJPO0y+0zmtMllKnOedOtI1uY8NF1GnMxOk+OS60yXGWeeFzmunTt3KrXM4WrcuLFSy1wvmdMl16k5J8wqN4/Icb7kfMhx6bYPuRzNYmNjlVoul+3btyv10aNHlVpmAsrtxTxtVhmNRPr9Vc633HZl/dNPPxmv5TLRrS/dcUw3bebto3Tp0kq3UqVKKXVUVJRSx8XFKbVcR3Ja5fq1Opbp9lf5Wbk+o6Ojlbp58+ZKnZqaarxev3690k3m8sn5lstYrk+r/ZmIKCIiwuWw5HbKzEotlwMAABRduCMOAAAAAAAAAADAC3AhDgAAAAAAAAAAwAtwIQ4AAAAAAAAAAMALEEYAAAB/erqMN8nTLDXz8HX5Yrpp0U2bLkPOTE63zPWS5LB0mUNWOW4yp6uw+XVW83m7WWXhEanzIudL91ndfMnlJpmztNLS0pRuMvNNjmv+/PmW0xYeHq7Uct7M+Vhy3cttS9YyA0zmXclxyeVgHp9cZiEhIUotu+/Zs0epZRaaHJfVPuxJXqScbme1LhNwy5YtxmuZEZefn6/U8tgg88lkd1n/+OOPSr1161bjtdzWLly4YDkuOa1y/cr8Qvl5q/1dbktyfcn5kutEZqvJ/ca8bRZ2uuW0JSUlKbXMnDPXugy/a9euEQAA/D3hjjgAAAAAAAAAAAAvwIU4AAAAAAAAAAAAL8CFOAAAAAAAAAAAAC9ARhwAAPzp6XLXdDlPus9bZanJvCJJZkLpMuNkZpE5k0iXR6XLVtJlZclpMQ9P9hsREUFW5LToloNVDpTMWZI5TXJ96nL85LTJaTGP29NxyWHJLDW5fuVyNGdSyek8d+6cZS23RV0um5w2c0aVnC85LKscRSKiqKgoy3HLdWoenhxXZGSkUst1oMv1kmR+nXl4VvufM7pjh8zlk9lq+/btc3tYctrksO69916lrlixolL/8ssvSn3q1CmXw5LLWC4zuT/rMiNlbR6+LndNDltu5+b5ICLKzMy0nFbz8EuWLKl0k/uEnBa5TOX6lfuszKerVauW8Vrm8r3xxhtKnZqaqtTezNEEAIA/Fo74AAAAAAAAAAAAXoALcQAAAAAAAAAAAF6AC3EAAAAAAAAAAABegIw4AAD405PZOZ5mxun6Nw/f05wembWk6241bXLcspbZSbrMMKvsO9ndKsNNTieRYy6TbrnJ4Zv7l+OSy0yOS86npMsQM0+Lp/Oly1KTn5d5VmYyt0u3/uR86abVKjtRt93q1qecVjkt6enpLqdFzpf8rCSXoZw2mcNntX3o5lvS5bbJcfv7+yv1kSNHjNcyh01mo8n5kt3r1aun1I0aNVLq3bt3K7U5E1AOS5LLRW5rMhtNZgDKfdhqH5PDkuOS/ctlLuvDhw8r9YkTJ4zXFy9eVLrdddddSi2XacOGDZVaztfatWuVevPmzUo9efJk47Vuf9UdzwEAoOjCHXEAAAAAAAAAAABegAtxAAAAAAAAAAAAXoALcQAAAAAAAAAAAF6AjDgAgCJM5v7oMsisstJ0OWu67CX5eU/Grat145I8yYHTzZcnOWy64euWkS7nSWYQ6bK3rLK0zPlSt0JOu6fZXGZyGeqGFR4e7vawPc0b9LS71bamW0ay1m1LumnRfb4wCjNs3bYmM8EkmTnmCbmM5bDkMpX7zJ49e5Ra5sCZ6bLx5P4ta12OY2RkpFJHRUUZr2Wmm9xH5GeLFy+u1L6+vkp98uRJpZbzbd62dTmKcj7ktNWoUUOpY2JilHrhwoVK/fvvvxuvT58+rXQz58cREW3atEmpV6xYodTmZUhElJ+fr9Ryvs3zosuy9DSPFAAAig60AAAAAAAAAAAAAF6AC3EAAAAAAAAAAABegAtxAAAAAAAAAAAAXmBjZv6jJwIAAO6MChUqKHVRyYgrbG5XYfs38zTnx6r/wkzHn11RzUMqyuusKJLrS3dckvllMrdNZoTJjDlzTpjMDJPDvnTpklJnZmYqdUREhFLL3Df5+bi4OON169atlW4yZ02Kj49XajnfGzdutKz/97//Ga/lMpa5a7LtkPPVt29fy89PnjxZqc25fVYZbkSO6yQnJ0ep69atq9SjRo1S6lOnTin1uHHjjNcy+1DOp8zKk8tp7969BAAARVPRPCsGAAAAAAAAAAD4k8GFOAAAAAAAAAAAAC+4c79jDwAAfzhPHwe8k4/ZFebRRDz+BwB3gjwu+fn5WXaXjxPK/uWjjmbysVU5LPnoohzWuXPnlFo+uiofHz158qTxeubMmZbjktMixy3nUz7CKYdnfgRUdpOPc0q6x4V1j/ia6/DwcKWbXEbys5Kc1rlz5yp18eLFlbpmzZrG6507dyrdzI/MEunnAwAAii7cEQcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AU2ZuY/eiIAAODOqFKlyh89CXeELm/O00y5wmTQeZp9Z9V/Uc7CK0xG4J9ZUV5nRZHMOpPrT2al6eqwsDCllrlf5v51w5Kflflmclp1WWrmbLZLly6RFZlPJnPdJDku+XlzFpucz+zsbKWW60SS2XhSRESEy3HL6crKyrIcti7zLyoqSqnbtGmj1M8//7zxetasWUq3Dz/80HLYctx79+4lAAAomormWTEAAAAAAAAAAMCfDC7EAQAAAAAAAAAAeAEuxAEAAAAAAAAAAHgBMuIAAIowZMS5Bxlxdx4y4uDPQJcRd/XqVcta5p2FhoYqtTmfjMh6u9flsMlpldlqspbDM09bUlKS0k3OV3p6ulLLfDpZy3HJaUlLSzNeyxy9ihUrKrUut02XwyeXsTkPTw4rPj5eqWNiYizHLWuZtSenLSEhwXh96tQppZus5bjltoSMOACAoqtonhUDAAAAAAAAAAD8yeBCHAAAAAAAAAAAgBfgQhwAAAAAAAAAAIAX+Ol7AQCAv6rCZKl5mn0l84w8ZR6fbtxFNW8MAG4/8/FE5pHJY4nMQouKilJqeZyTw5MZYuZxy8/q8uRkDpvsLvPJZMaYOUstJydH6SYz3WRmnMwzO336tFJHREQodbFixZTanMUmp0tOt1wuMgNOt4xl7puZzHiTWXhyOchxy2mV8y0zA0+cOGG8lstcl20HAAB/H2gBAAAAAAAAAAAAvAAX4gAAAAAAAAAAALwAF+IAAAAAAAAAAAC8ABlxAABFmMyg0WWvWeW0FTbPxtNpsXK7pw0AwBmZfSYzw2RemcxaMx+bZBaarGXemMxGi42NVerIyEilvnjxostpk3llMvuuQYMGSq3LiNPl2ZmP0TKX7fDhw0otM+Sssu6IHJebXObmaZHDktMth125cmWXwyIiWr9+veW4zXl1MrvO020HAACKLvznAgAAAAAAAAAA4AW4EAcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBTZm5j96IgAAAAAAAAAAAIo63BEHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AW4EAcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AW4EAcAAAAAAAAAAOAFuBAHAAAAAAAAAADgBbgQBwAAAAAAAAAA4AW4EAcAAAAAAAAAAOAF/w9xupn1Sb13RQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Get a sample\n",
        "image_tensor, encoded_label = dataset[3]  # You can change the index here (e.g., dataset[10])\n",
        "\n",
        "# Convert tensor to image (CHW to HWC)\n",
        "image_np = image_tensor.squeeze().numpy()  # Shape: [32, 128] for grayscale\n",
        "\n",
        "# Decode the label from indices to characters\n",
        "vocab = dataset.get_vocab()\n",
        "pad_token = vocab.index(\"<PAD>\")\n",
        "\n",
        "# Remove padding tokens and decode\n",
        "decoded_label = ''.join([vocab[i] for i in encoded_label]) # if i != pad_token])\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_np, cmap='gray')\n",
        "plt.title(f\"Label: {decoded_label}\")\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpRnBFFGLEQ8"
      },
      "source": [
        "## Train-Test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSGfkOGnFlVe"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.7 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn_wrapper(dataset)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    collate_fn=collate_fn_wrapper(dataset)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzPKZnuHFqLp"
      },
      "source": [
        "# Model architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1uy0YuZzGi6"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ca7eO5V5lan"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RcAddLvY5kcE"
      },
      "outputs": [],
      "source": [
        "# === Encoder: CNN with skip connections and progressive downsampling ===\n",
        "class Encoder_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Block 1: Conv + ReLU + MaxPool\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Block 2: Conv + ReLU + MaxPool\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "\n",
        "        # Block 3: Conv + BatchNorm + ReLU\n",
        "        self.block3 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # Skip connection from block2 to block3\n",
        "        self.skip23 = nn.Conv2d(128, 256, kernel_size=1)\n",
        "\n",
        "        # Block 4: Conv + ReLU + MaxPool (with (2,1) to retain width info)\n",
        "        self.block4 = nn.Sequential(\n",
        "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1))\n",
        "        )\n",
        "\n",
        "        # Block 5: Conv + BatchNorm + ReLU + MaxPool\n",
        "        self.block5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(2, 1))\n",
        "        )\n",
        "\n",
        "        # Skip connection from block4 to block5\n",
        "        self.skip45 = nn.Conv2d(256, 512, kernel_size=1)\n",
        "\n",
        "        # Final conv layer to flatten height dimension\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(512, 512, kernel_size=(2, 1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.block1(x)\n",
        "        x2 = self.block2(x1)\n",
        "\n",
        "        x3 = self.block3(x2)\n",
        "        skip_x2 = self.skip23(x2)\n",
        "        x3 = x3 + F.interpolate(skip_x2, size=x3.shape[2:], mode='nearest')\n",
        "\n",
        "        x4 = self.block4(x3)\n",
        "\n",
        "        x5 = self.block5(x4)\n",
        "        skip_x4 = self.skip45(x4)\n",
        "        x5 = x5 + F.interpolate(skip_x4, size=x5.shape[2:], mode='nearest')\n",
        "\n",
        "        x = self.final_conv(x5)  # [B, 512, H=1, W]\n",
        "        x = x.squeeze(2)         # [B, 512, W]\n",
        "        x = x.permute(0, 2, 1)   # [B, W, 512] - sequence of W steps with 512-dim features\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnFRQT6vQUSw"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlZ4ack_1Vpp"
      },
      "source": [
        "### Option 1: GRU -> Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vogKNVK-FsT1"
      },
      "outputs": [],
      "source": [
        "# === Decoder: GRU -> Attention ===\n",
        "class Decoder_GRU_Attention(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        self.attn = nn.MultiheadAttention(hidden_size, num_heads=1, batch_first=True)\n",
        "        self.attn_combine = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # [B, 1, H]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        gru_output, hidden = self.gru(embedded, hidden)  # [B, 1, H]\n",
        "        context, attn_weights = self.attn(gru_output, encoder_outputs, encoder_outputs)  # [B, 1, H]\n",
        "\n",
        "        combined = torch.cat((gru_output, context), dim=2)  # [B, 1, 2H]\n",
        "        combined = self.attn_combine(combined)              # [B, 1, H]\n",
        "        output = self.out(combined)                         # [B, 1, V]\n",
        "        return output.squeeze(1), hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPAneJUNyyWT"
      },
      "source": [
        "### Option 2: Attention -> GRU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1Uj8Ys_zB5L"
      },
      "outputs": [],
      "source": [
        "# === Decoder: Attention -> GRU ===\n",
        "class Decoder_Attention_GRU(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # Multi-head attention before feeding into GRU\n",
        "        self.attn = nn.MultiheadAttention(hidden_size, num_heads=1, batch_first=True)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input_token, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)  # [B, 1, H]\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        # Attention: query = embedded, key/value = encoder_outputs\n",
        "        context, attn_weights = self.attn(embedded, encoder_outputs, encoder_outputs)  # [B, 1, H]\n",
        "\n",
        "        # Feed attended context to GRU\n",
        "        output, hidden = self.gru(context, hidden)\n",
        "        output = self.out(output)  # [B, 1, V]\n",
        "        return output.squeeze(1), hidden, attn_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JtQr0LOjuYJ"
      },
      "source": [
        "### Option 3: TransformerDecoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkz7VM-Eju4i"
      },
      "outputs": [],
      "source": [
        "# === Transformer Decoder: with learnable positional encoding (properly initialized) ===\n",
        "class Transformer_Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_dim=512, nhead=8, num_layers=3, dropout=0.1, max_len=500):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n",
        "\n",
        "        # === FIX: Learnable positional encodings are now properly initialized ===\n",
        "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, hidden_dim))  # [1, T, H]\n",
        "        nn.init.normal_(self.positional_encoding, mean=0.0, std=0.02)  # ð¡ Proper init instead of zeros\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=hidden_dim, nhead=nhead, dim_feedforward=2048, dropout=dropout, batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_tokens, memory, tgt_mask=None, memory_mask=None):\n",
        "        B, T_out = tgt_tokens.shape\n",
        "\n",
        "        # Embedding + Positional Encoding + Dropout\n",
        "        x = self.embedding(tgt_tokens)                          # [B, T, H]\n",
        "        x = x + self.positional_encoding[:, :T_out, :]         # ð¡ Add positional encoding\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Transformer decoder expects (B, T, H); tgt_mask must be causal\n",
        "        output = self.transformer_decoder(\n",
        "            tgt=x,\n",
        "            memory=memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=memory_mask\n",
        "        )\n",
        "\n",
        "        output = self.fc_out(output)  # [B, T, V]\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "taO9UhZuBP_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === Set up wandb run name and architecture string ===\n",
        "if model_selected == 1:\n",
        "    architecture = \"Encoder: CNN + Decoder: GRU with Attention\"\n",
        "    wandb_name = f\"Seq2Seq: CNN-GRU-Attention-{num_images}_imgs\"\n",
        "elif model_selected == 2:\n",
        "    architecture = \"Encoder: CNN + Decoder: Attention then GRU\"\n",
        "    wandb_name = f\"Seq2Seq: CNN-Attention-GRU-{num_images}_imgs\"\n",
        "elif model_selected == 3:\n",
        "    architecture = \"Encoder: CNN + Decoder: Transformer\"\n",
        "    wandb_name = f\"Seq2Seq: CNN-Transformer-{num_images}_imgs\"\n",
        "\n",
        "# === wandb config ===\n",
        "config = {\n",
        "    \"epochs\":15,\n",
        "    \"batch_size\": 32,\n",
        "    \"learning_rate\":  0.001,\n",
        "    \"architecture\": architecture\n",
        "}\n",
        "wandb.init(project=\"OCR\", name=wandb_name)"
      ],
      "metadata": {
        "id": "KBLue0UMBSax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqAPdpOUFvgN"
      },
      "source": [
        "## Loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-iS1OkuFx_8"
      },
      "outputs": [],
      "source": [
        "# === Instantiate encoder and decoder based on model choice ===\n",
        "if model_selected == 1:\n",
        "    # Baseline: CNN encoder with GRU + additive attention decoder\n",
        "    encoder = Encoder_CNN().to(device)\n",
        "    decoder = Decoder_GRU_Attention(hidden_size=512, output_size=vocab_size).to(device)\n",
        "elif model_selected == 2:\n",
        "    # Variant: CNN encoder + attention-first decoder with GRU\n",
        "    encoder = Encoder_CNN().to(device)\n",
        "    decoder = Decoder_Attention_GRU(hidden_size=512, output_size=vocab_size).to(device)\n",
        "elif model_selected == 3:\n",
        "    # Transformer-based decoder over CNN encoder\n",
        "    encoder = Encoder_CNN().to(device)\n",
        "    decoder = Transformer_Decoder(\n",
        "        vocab_size=vocab_size,\n",
        "        hidden_dim=512,\n",
        "        nhead=8,\n",
        "        num_layers=3,\n",
        "        dropout=0.1,\n",
        "        max_len=500\n",
        "    ).to(device)\n",
        "\n",
        "# === Loss Function ===\n",
        "# Using CrossEntropyLoss, which expects class indices.\n",
        "# No padding ignored here by default â consider `ignore_index=token2idx[\"<PAD>\"]` for padding.\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=token2idx[\"<PAD>\"])\n",
        "\n",
        "# === Optimizer ===\n",
        "# Combine encoder and decoder parameters for joint optimization\n",
        "params = list(encoder.parameters()) + list(decoder.parameters())\n",
        "optimizer = torch.optim.Adam(params, lr=0.001)\n",
        "\n",
        "# Define learning rate scheduler to reduce LR every 5 epochs by factor 0.5 for stable convergence\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77H1TEhiFzPQ"
      },
      "source": [
        "## Train one epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sWFN1ttSF1BW"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch_attention(encoder, decoder, dataloader, optimizer, criterion, vocab, max_len=32):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        batch_size = imgs.size(0)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # === Forward pass ===\n",
        "        encoder_outputs = encoder(imgs)  # shape: [B, T_enc, H]\n",
        "        decoder_input = torch.full((batch_size,), vocab[\"<SOS>\"], dtype=torch.long, device=device)\n",
        "        decoder_hidden = None\n",
        "        loss = 0\n",
        "\n",
        "        # Teacher forcing: feed ground-truth label at each timestep\n",
        "        for t in range(labels.size(1)):\n",
        "            decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, labels[:, t])\n",
        "\n",
        "            # Teacher forcing decay\n",
        "            teacher_forcing_ratio = max(0.5, 1.0 - epoch * 0.1)  # Ejemplo: empieza en 1.0 y baja 0.1 por epoch\n",
        "\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            if use_teacher_forcing:\n",
        "                decoder_input = labels[:, t]  # ground truth\n",
        "            else:\n",
        "                top1 = decoder_output.argmax(1)\n",
        "                decoder_input = top1.detach()  # model prediction\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item() / labels.size(1)  # Normalize by sequence length\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    wandb.log({\"Train Loss\": avg_loss})\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "\n",
        "def train_one_epoch_transformer(encoder, decoder, dataloader, optimizer, criterion, vocab, max_len=32):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    pad_idx = vocab.get(\"<PAD>\", None)  # <-- FIX here\n",
        "\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        batch_size = imgs.size(0)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs = encoder(imgs)\n",
        "\n",
        "        tgt_input = labels[:, :-1]\n",
        "        tgt_output = labels[:, 1:]\n",
        "        tgt_seq_len = tgt_input.size(1)\n",
        "\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
        "\n",
        "        output = decoder(tgt_input, memory=encoder_outputs, tgt_mask=tgt_mask)\n",
        "\n",
        "        output = output.reshape(-1, output.size(-1))\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        if pad_idx is not None and hasattr(criterion, 'ignore_index'):\n",
        "            criterion.ignore_index = pad_idx\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    wandb.log({\"Train Loss\": avg_loss})\n",
        "    return avg_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-MUmhKkFr_1"
      },
      "source": [
        "## Sanity-check before training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8we5tivEFDX_",
        "outputId": "ba11f4b6-56e0-4be3-fe71-be1dcac9dbdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample 0: image shape=torch.Size([1, 32, 128]), label indices=[1, 15, 27, 33, 26, 32, 17, 30, 32, 17, 26, 27, 30, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded label: ['<SOS>', 'C', 'O', 'U', 'N', 'T', 'E', 'R', 'T', 'E', 'N', 'O', 'R', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "Sample 1: image shape=torch.Size([1, 32, 128]), label indices=[1, 28, 56, 47, 51, 39, 41, 63, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded label: ['<SOS>', 'P', 'r', 'i', 'm', 'a', 'c', 'y', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "Sample 2: image shape=torch.Size([1, 32, 128]), label indices=[1, 18, 59, 58, 59, 56, 53, 50, 53, 45, 63, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded label: ['<SOS>', 'F', 'u', 't', 'u', 'r', 'o', 'l', 'o', 'g', 'y', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "Sample 3: image shape=torch.Size([1, 32, 128]), label indices=[1, 35, 39, 58, 41, 46, 43, 56, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded label: ['<SOS>', 'W', 'a', 't', 'c', 'h', 'e', 'r', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n",
            "Sample 4: image shape=torch.Size([1, 32, 128]), label indices=[1, 31, 50, 59, 57, 46, 63, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Decoded label: ['<SOS>', 'S', 'l', 'u', 's', 'h', 'y', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
          ]
        }
      ],
      "source": [
        "############## Validations\n",
        "\n",
        "for i in range(5):\n",
        "    img, label = dataset[i]\n",
        "    print(f\"Sample {i}: image shape={img.shape}, label indices={label}\")\n",
        "    print(f\"Decoded label: {[idx2token[idx] for idx in label]}\") # if idx != token2idx['<PAD>']]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "zqHU3NCJFE1H",
        "outputId": "c4d79720-424c-4145-cb6d-509f79de51c0"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_loader' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3235940955.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Labels max value:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vocab size:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Etiqueta fuera del rango del vocabulario\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "for imgs, labels in train_loader:\n",
        "    print(\"Labels max value:\", labels.max().item())\n",
        "    print(\"Vocab size:\", vocab_size)\n",
        "    assert labels.max().item() < vocab_size, \"Labels out of vocabulary range\"\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY4kmC2CQwTD"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "LyOePsJMRNh3",
        "outputId": "8db45b09-492d-4716-ce93-ff6e9e63573e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "updating run metadata (2.3m)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 409: run g9e0famo was previously created and deleted; try a new run name"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "best_loss = float('inf')\n",
        "\n",
        "# === Training loop ===\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "    print(f\"\\nEpoch {epoch + 1}/{config['epochs']}\")\n",
        "\n",
        "    if model_selected in [1, 2]:\n",
        "        train_loss = train_one_epoch_attention(encoder, decoder, train_loader, optimizer, criterion, token2idx)\n",
        "    else:  # Transformer decoder\n",
        "        train_loss = train_one_epoch_transformer(encoder, decoder, train_loader, optimizer, criterion, token2idx)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # === Validation ===\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_gts = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in val_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = imgs.size(0)\n",
        "\n",
        "            encoder_outputs = encoder(imgs)\n",
        "\n",
        "            if model_selected in [1, 2]:  # GRU variants\n",
        "                decoder_input = torch.full((batch_size,), token2idx[\"<SOS>\"], dtype=torch.long, device=device)\n",
        "                decoder_hidden = None\n",
        "\n",
        "                preds_batch = []\n",
        "                for t in range(labels.size(1)):\n",
        "                    decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                    val_loss += criterion(decoder_output, labels[:, t]).item()\n",
        "\n",
        "                    # Get top-1 prediction\n",
        "                    top1 = decoder_output.argmax(1)\n",
        "                    preds_batch.append(top1)\n",
        "\n",
        "                    top1 = decoder_output.argmax(1)\n",
        "                    decoder_input = top1.detach()\n",
        "\n",
        "\n",
        "                # Convert predicted token IDs to strings\n",
        "                preds_batch = torch.stack(preds_batch, dim=1)  # [B, T]\n",
        "                preds_strs = [decode_sequence(seq, idx2token) for seq in preds_batch]\n",
        "                gts_strs = [decode_sequence(seq, idx2token) for seq in labels]\n",
        "\n",
        "\n",
        "                all_preds.extend(preds_strs)\n",
        "                all_gts.extend(gts_strs)\n",
        "\n",
        "            else:  # Transformer decoder\n",
        "\n",
        "                batch_size = imgs.size(0)\n",
        "                max_length = labels.size(1) + 10  # Safety margin\n",
        "\n",
        "                # === Inference decoding (greedy autoregressive) ===\n",
        "                decoded_sequences = [[] for _ in range(batch_size)]\n",
        "\n",
        "                # Initialize decoder input with <SOS> token\n",
        "                decoder_input = torch.full(\n",
        "                    (batch_size, 1),\n",
        "                    token2idx[\"<SOS>\"],\n",
        "                    dtype=torch.long,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                for _ in range(max_length):\n",
        "                    tgt_mask = generate_square_subsequent_mask(decoder_input.size(1), device=device)\n",
        "\n",
        "                    # Forward pass through transformer decoder\n",
        "                    output = decoder(\n",
        "                        decoder_input,\n",
        "                        memory=encoder_outputs,\n",
        "                        tgt_mask=tgt_mask\n",
        "                    )  # [B, T, V]\n",
        "\n",
        "                    # Get last timestep predictions (greedy decoding)\n",
        "                    next_token_logits = output[:, -1, :]  # [B, V]\n",
        "                    next_token = next_token_logits.argmax(dim=-1, keepdim=True)  # [B, 1]\n",
        "\n",
        "                    # Append predicted tokens to sequences\n",
        "                    for i in range(batch_size):\n",
        "                        decoded_sequences[i].append(next_token[i].item())\n",
        "\n",
        "                    # Concatenate next token to decoder input for next step\n",
        "                    decoder_input = torch.cat([decoder_input, next_token], dim=1)\n",
        "\n",
        "                    # Optional: stop if all sequences predicted <EOS>\n",
        "                    # Uncomment if you want early stopping for efficiency\n",
        "                    # if all(token2idx[\"<EOS>\"] in seq for seq in decoded_sequences):\n",
        "                    #     break\n",
        "\n",
        "                # === Calculate loss using teacher forcing ===\n",
        "                tgt_input = labels[:, :-1]\n",
        "                tgt_output = labels[:, 1:]\n",
        "                tgt_seq_len = tgt_input.size(1)\n",
        "                tgt_mask = generate_square_subsequent_mask(tgt_seq_len).to(device)\n",
        "\n",
        "                output = decoder(tgt_input, memory=encoder_outputs, tgt_mask=tgt_mask)\n",
        "                output = output.reshape(-1, output.size(-1))\n",
        "                tgt_output = tgt_output.reshape(-1)\n",
        "                val_loss += criterion(output, tgt_output).item()\n",
        "\n",
        "                # === Decode predictions and GTs to string ===\n",
        "                preds_strs = [decode_sequence(seq, idx2token) for seq in decoded_sequences]\n",
        "                gts_strs = [decode_sequence(seq, idx2token) for seq in labels]\n",
        "\n",
        "                all_preds.extend(preds_strs)\n",
        "                all_gts.extend(gts_strs)\n",
        "\n",
        "\n",
        "    val_loss /= (len(val_loader) * labels.size(1))\n",
        "\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # === Character Error Rate (CER) ===\n",
        "    cer = character_error_rate(all_preds, all_gts)\n",
        "    print(f\"CER: {cer:.4f}\")\n",
        "\n",
        "    # === Word Accuracy@k ===\n",
        "    acc_k_dict = {}\n",
        "    for k in [0, 1, 2]:\n",
        "        acc_k = word_accuracy_at_k(all_preds, all_gts, k)\n",
        "        acc_k_dict[f\"Word Accuracy @{k}\"] = acc_k\n",
        "        print(f\"Word Accuracy @{k}: {acc_k:.4f}\")\n",
        "\n",
        "    # === Learning Rate Logging ===\n",
        "    lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Learning Rate: {lr:.6f}\")\n",
        "\n",
        "    # === Log to wandb ===\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch + 1,\n",
        "        \"Train Loss\": train_loss,\n",
        "        \"Val Loss\": val_loss,\n",
        "        \"CER\": cer,\n",
        "        **acc_k_dict,\n",
        "        \"LR\": lr,\n",
        "    })\n",
        "\n",
        "    # === Step the learning rate scheduler ===\n",
        "    scheduler.step()\n",
        "\n",
        "    # === Save best model ===\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save({\n",
        "            \"encoder\": encoder.state_dict(),\n",
        "            \"decoder\": decoder.state_dict(),\n",
        "            \"vocab\": vocab,  # optional\n",
        "        }, \"OCR_seq_to_seq.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJS8XMWeF4NO"
      },
      "source": [
        "# Inference & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_predictions(encoder, decoder, dataset, token2idx, num_samples=10)"
      ],
      "metadata": {
        "id": "NwokZCoPBrNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjWzqRu27CMz"
      },
      "source": [
        "## Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_params = count_parameters(encoder)\n",
        "decoder_params = count_parameters(encoder)\n",
        "total_params = encoder_params + decoder_params\n",
        "\n",
        "print(f\"Number of Encoder parameters: {encoder_params:,}\")\n",
        "print(f\"Number of Decoder parameters: {decoder_params:,}\")\n",
        "\n",
        "wandb.log({\"Num Params\": total_params})"
      ],
      "metadata": {
        "id": "zuw6li6GBvZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation"
      ],
      "metadata": {
        "id": "HZgHMihSBw2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Token level metrics... word levels was as well"
      ],
      "metadata": {
        "id": "8cCC4_boBzpC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq2auCX27Pdr"
      },
      "outputs": [],
      "source": [
        "def final_precision_recall_auc(encoder, decoder, val_loader, inv_vocab, token2idx):\n",
        "    \"\"\"\n",
        "    Computes Precision-Recall curve and AUC for seq2seq model (per-sample evaluation),\n",
        "    logs results to wandb, and saves the PR plot as an image artifact.\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = images.size(0)\n",
        "\n",
        "            encoder_outputs = encoder(images)\n",
        "\n",
        "            # Initialize decoder input (<SOS>)\n",
        "            decoder_input = torch.full((batch_size,), token2idx[\"<SOS>\"], dtype=torch.long, device=device)\n",
        "            decoder_hidden = None\n",
        "            preds_batch = []\n",
        "            confidences_batch = []\n",
        "\n",
        "            for t in range(labels.size(1)):\n",
        "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                probs = torch.softmax(decoder_output, dim=1)\n",
        "                top1 = probs.argmax(1)\n",
        "                confidence = probs.max(1).values.cpu().numpy()\n",
        "\n",
        "                preds_batch.append(top1)\n",
        "                confidences_batch.append(confidence)\n",
        "\n",
        "                decoder_input = top1  # Autoregressive decoding\n",
        "\n",
        "            # Convert predictions to strings\n",
        "            preds_batch = torch.stack(preds_batch, dim=1)\n",
        "            preds_strs = [decode_sequence(seq, inv_vocab) for seq in preds_batch]\n",
        "            gts_strs = [decode_ground_truth(seq, inv_vocab) for seq in labels]\n",
        "\n",
        "            # Mean confidence per sample (per-sample evaluation)\n",
        "            confidences_batch = np.stack(confidences_batch, axis=1)  # [B, T]\n",
        "            mean_confidences = confidences_batch.mean(axis=1)\n",
        "\n",
        "            sample_correctness = [int(p == g) for p, g in zip(preds_strs, gts_strs)]\n",
        "            y_true.extend(sample_correctness)\n",
        "            y_scores.extend(mean_confidences)\n",
        "\n",
        "    # Compute PR curve and AUC\n",
        "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
        "    pr_auc = auc(recall, precision)\n",
        "\n",
        "    # Plot and save PR curve\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(recall, precision, marker='.', label=f'AUC={pr_auc:.2f}')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve (Seq2Seq)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
        "        plt.savefig(tmpfile.name)\n",
        "        wandb.log({\"Precision-Recall Curve\": wandb.Image(tmpfile.name)})\n",
        "    plt.close()\n",
        "\n",
        "    wandb.log({\"Precision-Recall AUC\": pr_auc})\n",
        "\n",
        "    print(f\"Final Precision-Recall AUC: {pr_auc:.4f}\")\n",
        "    return pr_auc\n",
        "\n",
        "\n",
        "def final_roc_auc(encoder, decoder, val_loader, inv_vocab, token2idx):\n",
        "    \"\"\"\n",
        "    Computes ROC AUC for seq2seq model (per-sample evaluation),\n",
        "    logs results to wandb, and saves the ROC plot as an image artifact.\n",
        "    \"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = images.size(0)\n",
        "\n",
        "            encoder_outputs = encoder(images)\n",
        "\n",
        "            # Initialize decoder input (<SOS>)\n",
        "            decoder_input = torch.full((batch_size,), token2idx[\"<SOS>\"], dtype=torch.long, device=device)\n",
        "            decoder_hidden = None\n",
        "            preds_batch = []\n",
        "            confidences_batch = []\n",
        "\n",
        "            for t in range(labels.size(1)):\n",
        "                decoder_output, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "\n",
        "                probs = torch.softmax(decoder_output, dim=1)\n",
        "                top1 = probs.argmax(1)\n",
        "                confidence = probs.max(1).values.cpu().numpy()\n",
        "\n",
        "                preds_batch.append(top1)\n",
        "                confidences_batch.append(confidence)\n",
        "\n",
        "                decoder_input = top1  # Autoregressive decoding\n",
        "\n",
        "            # Convert predictions to strings\n",
        "            preds_batch = torch.stack(preds_batch, dim=1)\n",
        "            preds_strs = [decode_sequence(seq, inv_vocab) for seq in preds_batch]\n",
        "            gts_strs = [decode_ground_truth(seq, inv_vocab) for seq in labels]\n",
        "\n",
        "            # Mean confidence per sample (per-sample evaluation)\n",
        "            confidences_batch = np.stack(confidences_batch, axis=1)  # [B, T]\n",
        "            mean_confidences = confidences_batch.mean(axis=1)\n",
        "\n",
        "            sample_correctness = [int(p == g) for p, g in zip(preds_strs, gts_strs)]\n",
        "            y_true.extend(sample_correctness)\n",
        "            y_scores.extend(mean_confidences)\n",
        "\n",
        "    # Compute ROC curve and AUC\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = roc_auc_score(y_true, y_scores)\n",
        "\n",
        "    # Plot and save ROC curve\n",
        "    plt.figure(figsize=(6,5))\n",
        "    plt.plot(fpr, tpr, marker='.', label=f'AUC={roc_auc:.2f}')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve (Seq2Seq)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmpfile:\n",
        "        plt.savefig(tmpfile.name)\n",
        "        wandb.log({\"ROC Curve\": wandb.Image(tmpfile.name)})\n",
        "    plt.close()\n",
        "\n",
        "    wandb.log({\"ROC AUC\": roc_auc})\n",
        "\n",
        "    print(f\"Final ROC AUC (Seq2Seq): {roc_auc:.4f}\")\n",
        "    return roc_auc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6Iap4Ovsadp"
      },
      "outputs": [],
      "source": [
        "final_precision_recall_auc(encoder, decoder, val_loader, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-R8vrfIsf7k"
      },
      "outputs": [],
      "source": [
        "final_roc_auc(encoder, decoder, val_loader, vocab)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}